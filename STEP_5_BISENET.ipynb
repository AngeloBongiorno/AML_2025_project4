{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeloBongiorno/AML_2025_project4/blob/vito/STEP_5_BISENET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7QJviwzoN5m"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHpwU5g8s-Pt"
      },
      "source": [
        "## Upload .zip files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2_aVBiH46K3"
      },
      "source": [
        "For this step you must have the zip files in your Drive into a folder called `AML_project`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xxVbeep6Rlnb",
        "outputId": "09ada2fa-71f5-49ca-d69a-7694b5cad7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.11/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "z2LFD5EkeGs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b9bcf5-1c82-47f5-f4c8-2669b12bd1ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AML_2025_project4' already exists and is not an empty directory.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "!git clone -b angelo_albumentations --single-branch https://github.com/AngeloBongiorno/AML_2025_project4.git\n",
        "\n",
        "!cp AML_2025_project4/utils.py .\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CvdkrFwFI0Qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fc3736-adc3-4e4e-8a48-92d815834b2e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'utils' from '/content/utils.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import importlib\n",
        "import utils  # Replace with the actual module name\n",
        "\n",
        "importlib.reload(utils)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "laEb8KOytCpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560eb413-1fc3-416d-9b53-cfaa8e15bff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping extraction for the dataset, already extracted.\n",
            "{'training_urban': '/content/dataset/Train/Urban', 'training_rural': '/content/dataset/Train/Rural', 'validation_urban': '/content/dataset/Val/Urban', 'validation_rural': '/content/dataset/Val/Rural'}\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "from utils import get_loveDA\n",
        "\n",
        "paths = get_loveDA(verbose=True)\n",
        "print(paths)\n",
        "\n",
        "TRAINING_PATH_URBAN = paths[\"training_urban\"]\n",
        "TRAINING_PATH_RURAL = paths[\"training_rural\"]\n",
        "VAL_PATH_URBAN = paths[\"validation_urban\"]\n",
        "VAL_PATH_RURAL = paths[\"validation_rural\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEM_CLASSES = [\n",
        "    'background',\n",
        "    'building',\n",
        "    'road',\n",
        "    'water',\n",
        "    'barren',\n",
        "    'forest',\n",
        "    'agriculture'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(SEM_CLASSES)\n",
        "\n",
        "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(SEM_CLASSES)}\n",
        "\n",
        "IGNORE_INDEX = -1\n",
        "\n",
        "RESIZE = 512\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "STEP_SIZE = 21\n",
        "\n",
        "GAMMA = 0.5\n",
        "\n",
        "#LR = 0.00053\n",
        "LR = 0.0003\n",
        "\n",
        "P = 0.5 # probabilità augmentation\n",
        "\n",
        "ALPHA_TEACHER = 0.99\n",
        "\n",
        "THRESHOLD = 0.9\n",
        "\n",
        "MOMENTUM = 0.85\n",
        "\n",
        "LOSS_TYPE = \"\" # \"ohem\", \"ce\"\n",
        "\n",
        "TYPE_WEIGHT = \"inverse\" # median-frequency | inverse | log\n",
        "\n",
        "PIXEL_WEIGHT = \"tharshold_uniform\"\n",
        "\n",
        "\n",
        "WEIGHT_DECAY= 0.005\n",
        "NESTEROV = True"
      ],
      "metadata": {
        "id": "VJdiPeF5idkI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and instantiate"
      ],
      "metadata": {
        "id": "dAYUGwGYiFGi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrjECeMs7Sc5"
      },
      "source": [
        "### Define BISENetV2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.model_zoo as modelzoo\n",
        "\n",
        "backbone_url = 'https://github.com/CoinCheung/BiSeNet/releases/download/0.0.0/backbone_v2.pth'\n",
        "\n",
        "\n",
        "class ConvBNReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1,\n",
        "                 dilation=1, groups=1, bias=False):\n",
        "        super(ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(\n",
        "                in_chan, out_chan, kernel_size=ks, stride=stride,\n",
        "                padding=padding, dilation=dilation,\n",
        "                groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_chan)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv(x)\n",
        "        feat = self.bn(feat)\n",
        "        feat = self.relu(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "\n",
        "    def __init__(self, n_chan, factor=2):\n",
        "        super(UpSample, self).__init__()\n",
        "        out_chan = n_chan * factor * factor\n",
        "        self.proj = nn.Conv2d(n_chan, out_chan, 1, 1, 0)\n",
        "        self.up = nn.PixelShuffle(factor)\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.proj(x)\n",
        "        feat = self.up(feat)\n",
        "        return feat\n",
        "\n",
        "    def init_weight(self):\n",
        "        nn.init.xavier_normal_(self.proj.weight, gain=1.)\n",
        "\n",
        "\n",
        "\n",
        "class DetailBranch(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DetailBranch, self).__init__()\n",
        "        self.S1 = nn.Sequential(\n",
        "            ConvBNReLU(3, 64, 3, stride=2),\n",
        "            ConvBNReLU(64, 64, 3, stride=1),\n",
        "        )\n",
        "        self.S2 = nn.Sequential(\n",
        "            ConvBNReLU(64, 64, 3, stride=2),\n",
        "            ConvBNReLU(64, 64, 3, stride=1),\n",
        "            ConvBNReLU(64, 64, 3, stride=1),\n",
        "        )\n",
        "        self.S3 = nn.Sequential(\n",
        "            ConvBNReLU(64, 128, 3, stride=2),\n",
        "            ConvBNReLU(128, 128, 3, stride=1),\n",
        "            ConvBNReLU(128, 128, 3, stride=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.S1(x)\n",
        "        feat = self.S2(feat)\n",
        "        feat = self.S3(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class StemBlock(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(StemBlock, self).__init__()\n",
        "        self.conv = ConvBNReLU(3, 16, 3, stride=2)\n",
        "        self.left = nn.Sequential(\n",
        "            ConvBNReLU(16, 8, 1, stride=1, padding=0),\n",
        "            ConvBNReLU(8, 16, 3, stride=2),\n",
        "        )\n",
        "        self.right = nn.MaxPool2d(\n",
        "            kernel_size=3, stride=2, padding=1, ceil_mode=False)\n",
        "        self.fuse = ConvBNReLU(32, 16, 3, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv(x)\n",
        "        feat_left = self.left(feat)\n",
        "        feat_right = self.right(feat)\n",
        "        feat = torch.cat([feat_left, feat_right], dim=1)\n",
        "        feat = self.fuse(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class CEBlock(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CEBlock, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(128)\n",
        "        self.conv_gap = ConvBNReLU(128, 128, 1, stride=1, padding=0)\n",
        "        #TODO: in paper here is naive conv2d, no bn-relu\n",
        "        self.conv_last = ConvBNReLU(128, 128, 3, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = torch.mean(x, dim=(2, 3), keepdim=True)\n",
        "        feat = self.bn(feat)\n",
        "        feat = self.conv_gap(feat)\n",
        "        feat = feat + x\n",
        "        feat = self.conv_last(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class GELayerS1(nn.Module):\n",
        "\n",
        "    def __init__(self, in_chan, out_chan, exp_ratio=6):\n",
        "        super(GELayerS1, self).__init__()\n",
        "        mid_chan = in_chan * exp_ratio\n",
        "        self.conv1 = ConvBNReLU(in_chan, in_chan, 3, stride=1)\n",
        "        self.dwconv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_chan, mid_chan, kernel_size=3, stride=1,\n",
        "                padding=1, groups=in_chan, bias=False),\n",
        "            nn.BatchNorm2d(mid_chan),\n",
        "            nn.ReLU(inplace=True), # not shown in paper\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                mid_chan, out_chan, kernel_size=1, stride=1,\n",
        "                padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_chan),\n",
        "        )\n",
        "        self.conv2[1].last_bn = True\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv1(x)\n",
        "        feat = self.dwconv(feat)\n",
        "        feat = self.conv2(feat)\n",
        "        feat = feat + x\n",
        "        feat = self.relu(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class GELayerS2(nn.Module):\n",
        "\n",
        "    def __init__(self, in_chan, out_chan, exp_ratio=6):\n",
        "        super(GELayerS2, self).__init__()\n",
        "        mid_chan = in_chan * exp_ratio\n",
        "        self.conv1 = ConvBNReLU(in_chan, in_chan, 3, stride=1)\n",
        "        self.dwconv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_chan, mid_chan, kernel_size=3, stride=2,\n",
        "                padding=1, groups=in_chan, bias=False),\n",
        "            nn.BatchNorm2d(mid_chan),\n",
        "        )\n",
        "        self.dwconv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                mid_chan, mid_chan, kernel_size=3, stride=1,\n",
        "                padding=1, groups=mid_chan, bias=False),\n",
        "            nn.BatchNorm2d(mid_chan),\n",
        "            nn.ReLU(inplace=True), # not shown in paper\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                mid_chan, out_chan, kernel_size=1, stride=1,\n",
        "                padding=0, bias=False),\n",
        "            nn.BatchNorm2d(out_chan),\n",
        "        )\n",
        "        self.conv2[1].last_bn = True\n",
        "        self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_chan, in_chan, kernel_size=3, stride=2,\n",
        "                    padding=1, groups=in_chan, bias=False),\n",
        "                nn.BatchNorm2d(in_chan),\n",
        "                nn.Conv2d(\n",
        "                    in_chan, out_chan, kernel_size=1, stride=1,\n",
        "                    padding=0, bias=False),\n",
        "                nn.BatchNorm2d(out_chan),\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv1(x)\n",
        "        feat = self.dwconv1(feat)\n",
        "        feat = self.dwconv2(feat)\n",
        "        feat = self.conv2(feat)\n",
        "        shortcut = self.shortcut(x)\n",
        "        feat = feat + shortcut\n",
        "        feat = self.relu(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class SegmentBranch(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SegmentBranch, self).__init__()\n",
        "        self.S1S2 = StemBlock()\n",
        "        self.S3 = nn.Sequential(\n",
        "            GELayerS2(16, 32),\n",
        "            GELayerS1(32, 32),\n",
        "        )\n",
        "        self.S4 = nn.Sequential(\n",
        "            GELayerS2(32, 64),\n",
        "            GELayerS1(64, 64),\n",
        "        )\n",
        "        self.S5_4 = nn.Sequential(\n",
        "            GELayerS2(64, 128),\n",
        "            GELayerS1(128, 128),\n",
        "            GELayerS1(128, 128),\n",
        "            GELayerS1(128, 128),\n",
        "        )\n",
        "        self.S5_5 = CEBlock()\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat2 = self.S1S2(x)\n",
        "        feat3 = self.S3(feat2)\n",
        "        feat4 = self.S4(feat3)\n",
        "        feat5_4 = self.S5_4(feat4)\n",
        "        feat5_5 = self.S5_5(feat5_4)\n",
        "        return feat2, feat3, feat4, feat5_4, feat5_5\n",
        "\n",
        "\n",
        "class BGALayer(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BGALayer, self).__init__()\n",
        "        self.left1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=3, stride=1,\n",
        "                padding=1, groups=128, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=1, stride=1,\n",
        "                padding=0, bias=False),\n",
        "        )\n",
        "        self.left2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=3, stride=2,\n",
        "                padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.AvgPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)\n",
        "        )\n",
        "        self.right1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=3, stride=1,\n",
        "                padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "        self.right2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=3, stride=1,\n",
        "                padding=1, groups=128, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=1, stride=1,\n",
        "                padding=0, bias=False),\n",
        "        )\n",
        "        self.up1 = nn.Upsample(scale_factor=4)\n",
        "        self.up2 = nn.Upsample(scale_factor=4)\n",
        "        ##TODO: does this really has no relu?\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                128, 128, kernel_size=3, stride=1,\n",
        "                padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True), # not shown in paper\n",
        "        )\n",
        "\n",
        "    def forward(self, x_d, x_s):\n",
        "        dsize = x_d.size()[2:]\n",
        "        left1 = self.left1(x_d)\n",
        "        left2 = self.left2(x_d)\n",
        "        right1 = self.right1(x_s)\n",
        "        right2 = self.right2(x_s)\n",
        "        right1 = self.up1(right1)\n",
        "        left = left1 * torch.sigmoid(right1)\n",
        "        right = left2 * torch.sigmoid(right2)\n",
        "        right = self.up2(right)\n",
        "        out = self.conv(left + right)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class SegmentHead(nn.Module):\n",
        "\n",
        "    def __init__(self, in_chan, mid_chan, n_classes, up_factor=8, aux=True):\n",
        "        super(SegmentHead, self).__init__()\n",
        "        self.conv = ConvBNReLU(in_chan, mid_chan, 3, stride=1)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.up_factor = up_factor\n",
        "\n",
        "        out_chan = n_classes\n",
        "        mid_chan2 = up_factor * up_factor if aux else mid_chan\n",
        "        up_factor = up_factor // 2 if aux else up_factor\n",
        "        self.conv_out = nn.Sequential(\n",
        "            nn.Sequential(\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                ConvBNReLU(mid_chan, mid_chan2, 3, stride=1)\n",
        "                ) if aux else nn.Identity(),\n",
        "            nn.Conv2d(mid_chan2, out_chan, 1, 1, 0, bias=True),\n",
        "            nn.Upsample(scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.conv(x)\n",
        "        feat = self.drop(feat)\n",
        "        feat = self.conv_out(feat)\n",
        "        return feat\n",
        "\n",
        "\n",
        "class CustomArgMax(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, feat_out, dim):\n",
        "        return feat_out.argmax(dim=dim).int()\n",
        "\n",
        "    @staticmethod\n",
        "    def symbolic(g, feat_out, dim: int):\n",
        "        return g.op('CustomArgMax', feat_out, dim_i=dim)\n",
        "\n",
        "\n",
        "class BiSeNetV2(nn.Module):\n",
        "\n",
        "    def __init__(self, n_classes, aux_mode='train'):\n",
        "        super(BiSeNetV2, self).__init__()\n",
        "        self.aux_mode = aux_mode\n",
        "        self.detail = DetailBranch()\n",
        "        self.segment = SegmentBranch()\n",
        "        self.bga = BGALayer()\n",
        "\n",
        "        ## TODO: what is the number of mid chan ?\n",
        "        self.head = SegmentHead(128, 1024, n_classes, up_factor=8, aux=False)\n",
        "        if self.aux_mode == 'train':\n",
        "            self.aux2 = SegmentHead(16, 128, n_classes, up_factor=4)\n",
        "            self.aux3 = SegmentHead(32, 128, n_classes, up_factor=8)\n",
        "            self.aux4 = SegmentHead(64, 128, n_classes, up_factor=16)\n",
        "            self.aux5_4 = SegmentHead(128, 128, n_classes, up_factor=32)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        size = x.size()[2:]\n",
        "        feat_d = self.detail(x)\n",
        "        feat2, feat3, feat4, feat5_4, feat_s = self.segment(x)\n",
        "        feat_head = self.bga(feat_d, feat_s)\n",
        "\n",
        "        logits = self.head(feat_head)\n",
        "        if self.aux_mode == 'train':\n",
        "            logits_aux2 = self.aux2(feat2)\n",
        "            logits_aux3 = self.aux3(feat3)\n",
        "            logits_aux4 = self.aux4(feat4)\n",
        "            logits_aux5_4 = self.aux5_4(feat5_4)\n",
        "            return logits, logits_aux2, logits_aux3, logits_aux4, logits_aux5_4\n",
        "        elif self.aux_mode == 'eval':\n",
        "            return logits,\n",
        "        elif self.aux_mode == 'pred':\n",
        "            #  pred = logits.argmax(dim=1)\n",
        "            pred = CustomArgMax.apply(logits, 1)\n",
        "            return pred\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.kaiming_normal_(module.weight, mode='fan_out')\n",
        "                if not module.bias is None: nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n",
        "                if hasattr(module, 'last_bn') and module.last_bn:\n",
        "                    nn.init.zeros_(module.weight)\n",
        "                else:\n",
        "                    nn.init.ones_(module.weight)\n",
        "                nn.init.zeros_(module.bias)\n",
        "        print(\"Loading weights...\")\n",
        "        self.load_pretrain()\n",
        "        print(\"Loaded weights!!\")\n",
        "\n",
        "\n",
        "    def load_pretrain(self):\n",
        "        state = modelzoo.load_url(backbone_url)\n",
        "        for name, child in self.named_children():\n",
        "            if name in state.keys():\n",
        "                child.load_state_dict(state[name], strict=True)\n",
        "\n",
        "    def get_params(self):\n",
        "        def add_param_to_list(mod, wd_params, nowd_params):\n",
        "            for param in mod.parameters():\n",
        "                if param.dim() == 1:\n",
        "                    nowd_params.append(param)\n",
        "                elif param.dim() == 4:\n",
        "                    wd_params.append(param)\n",
        "                else:\n",
        "                    print(name)\n",
        "\n",
        "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
        "        for name, child in self.named_children():\n",
        "            if 'head' in name or 'aux' in name:\n",
        "                add_param_to_list(child, lr_mul_wd_params, lr_mul_nowd_params)\n",
        "            else:\n",
        "                add_param_to_list(child, wd_params, nowd_params)\n",
        "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n"
      ],
      "metadata": {
        "id": "qCrSeMYhpcqr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw9SYUCgi6us"
      },
      "source": [
        "# Dataset & dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset definition"
      ],
      "metadata": {
        "id": "wrMzI_LbjhP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T6kSW8hGjAo9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform, target=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
        "\n",
        "        # Read an image with OpenCV\n",
        "        image = cv2.imread(img_path)\n",
        "        mask = cv2.imread(mask_path)\n",
        "\n",
        "        # By default OpenCV uses BGR color space for color images,\n",
        "        # so we need to convert the image to RGB color space.\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "        edge = cv2.Canny(mask_np, 0.1, 0.2)\n",
        "\n",
        "        kernel = np.ones((3, 3), np.uint8)  # Kernel for dilation\n",
        "\n",
        "        edge = edge[6:-6, 6:-6]\n",
        "        edge = np.pad(edge, ((6,6),(6,6)), mode='constant')\n",
        "        boundaries = cv2.dilate(edge, kernel, iterations=1)  # Dilate edges\n",
        "        boundaries = (boundaries > 50) * 1.0 # boundaries matrix is float with 1.0 or 0.0\n",
        "\n",
        "        mask = torch.as_tensor(np.array(mask), dtype=torch.int64) - 1\n",
        "\n",
        "        boundaries_tensor = torch.as_tensor(boundaries, dtype=torch.float32)\n",
        "\n",
        "        # if the dataset is a target dataset, does not return the mask\n",
        "        if self.target == True:\n",
        "          return image, boundaries_tensor\n",
        "        return image, mask, boundaries_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for images & masks\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision.transforms import v2 as T\n",
        "import cv2\n",
        "\n",
        "resize_transform = A.Compose([\n",
        "    A.Resize(height=RESIZE, width=RESIZE, p=1),\n",
        "    A.ToFloat(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# the best augmentation from previous step is chosen\n",
        "augment =  A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=P)"
      ],
      "metadata": {
        "id": "cOr2yJ_6kvA4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_boundary_mask(mask):\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        mask_np = mask.squeeze().cpu().numpy()  # Assicura che sia [H, W]\n",
        "    else:\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "    mask_np = mask_np.astype(np.uint8)\n",
        "\n",
        "    edge = cv2.Canny(mask_np, 0.1, 0.2)\n",
        "\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "    edge = edge[6:-6, 6:-6]\n",
        "    edge = np.pad(edge, ((6,6),(6,6)), mode='constant')\n",
        "    boundaries = cv2.dilate(edge, kernel, iterations=1)\n",
        "    boundaries = (boundaries > 50).astype(np.float32)\n",
        "\n",
        "    boundaries_tensor = torch.from_numpy(boundaries).unsqueeze(0)  # shape: [1, H, W]\n",
        "    return boundaries_tensor\n"
      ],
      "metadata": {
        "id": "fbbVpaD-McpG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset instantiation"
      ],
      "metadata": {
        "id": "aRC4KXtmj3Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects\n",
        "\n",
        "# TRAINING DATASETS\n",
        "source_dataset = SegmentationDataset(TRAINING_PATH_URBAN + \"/images_png\", TRAINING_PATH_URBAN + \"/masks_png\",\n",
        "                                    transform=resize_transform)\n",
        "\n",
        "\n",
        "target_dataset = SegmentationDataset(TRAINING_PATH_RURAL + \"/images_png\", TRAINING_PATH_RURAL + \"/masks_png\",\n",
        "                                    transform=resize_transform, target=True)\n",
        "\n",
        "# EVALUATION DATASET\n",
        "\n",
        "val_dataset = SegmentationDataset(VAL_PATH_RURAL + \"/images_png\", VAL_PATH_RURAL + \"/masks_png\",\n",
        "                                    transform=resize_transform)"
      ],
      "metadata": {
        "id": "Zk4ZifehjuyZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Supponiamo tu abbia un DataLoader con le etichette GT (y) nei batch\n",
        "class_counts = torch.zeros(NUM_CLASSES)\n",
        "\n",
        "for (images, labels, _) in tqdm(DataLoader(source_dataset, batch_size=BATCH_SIZE)):\n",
        "    for c in range(NUM_CLASSES):\n",
        "        class_counts[c] += torch.sum(labels == c)\n",
        "\n",
        "# Converti in numpy\n",
        "class_counts = class_counts.numpy()\n",
        "total_pixels = np.sum(class_counts)\n",
        "frequencies = class_counts / total_pixels\n",
        "\n",
        "if TYPE_WEIGHT == \"inverse\":\n",
        "  #Inverse frequency\n",
        "  class_weights = 1.0 / (frequencies + 1e-8)\n",
        "elif TYPE_WEIGHT == \"median-frequency\":\n",
        "  #Median frequency balancing\n",
        "  median = np.median(frequencies)\n",
        "  class_weights = median / (frequencies + 1e-8)\n",
        "elif TYPE_WEIGHT == \"log\":\n",
        "  #Log smoothing\n",
        "  class_weights = 1.0 / np.log(1.02 + frequencies)\n",
        "\n",
        "print(class_weights)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-PvK8l4668q",
        "outputId": "9ddb66a9-6f05-4a96-cfed-d11aea9ef800"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 73/73 [01:02<00:00,  1.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.063954  4.717028 10.776062 26.797655 13.217381 12.630906 53.904175]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loader instantiation"
      ],
      "metadata": {
        "id": "lyTIKtzjj7i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "\n",
        "# TRAINING DATALOADERS\n",
        "source_loader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "# EVALUATION DATALOADERS\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# enumerate dataloaders\n",
        "source_loader_iter = enumerate(source_loader)\n",
        "target_loader_iter = enumerate(target_loader)\n"
      ],
      "metadata": {
        "id": "vhsQeNzTj_mk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4pcqQXrMzza"
      },
      "source": [
        "### Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "hZNWIM0DbnJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "262784c2-9647-4e6e-c7be-7348a1cab917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights...\n",
            "Loaded weights!!\n"
          ]
        }
      ],
      "source": [
        "#istanziare BISENET e applicare i pesi\n",
        "model = BiSeNetV2(NUM_CLASSES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSQUcy7_t2of"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define loss functions"
      ],
      "metadata": {
        "id": "p_wwWFwFkIoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# # Extra Semantic Loss (Classica CrossEntropy Loss)\n",
        "# class CrossEntropyLoss(nn.Module):\n",
        "#     def __init__(self, num_outputs, weight=None, balance_weights=[0.4, 1.0], sb_weights=1.0):\n",
        "#         super(CrossEntropyLoss, self).__init__()\n",
        "#         self.loss = nn.CrossEntropyLoss(weight=weight, ignore_index=IGNORE_INDEX)\n",
        "#         self.num_outputs = num_outputs\n",
        "#         self.balance_weights = balance_weights\n",
        "#         self.sb_weights = sb_weights\n",
        "\n",
        "#     def _forward(self, pred, target):\n",
        "#         return self.loss(pred, target)\n",
        "\n",
        "#     def forward(self, score, target):\n",
        "#         if self.num_outputs == 1:\n",
        "#             score = [score]\n",
        "\n",
        "#         if len(self.balance_weights) == len(score):\n",
        "#             return sum([w * self._forward(x, target) for (w, x) in zip(self.balance_weights, score)])\n",
        "#         elif len(score) == 1:\n",
        "#             return self.sb_weights * self._forward(score[0], target)\n",
        "#         else:\n",
        "#             raise ValueError(\"lengths of prediction and target are not identical!\")\n",
        "\n",
        "# class OhemCrossEntropy(nn.Module):\n",
        "#     def __init__(self, thres=0.7, min_kept=26_000, balance_weights=[0.4, 1.0], sb_weights=1.0, weight=None):\n",
        "#         super(OhemCrossEntropy, self).__init__()\n",
        "#         self.thresh = thres\n",
        "#         self.min_kept = max(1, min_kept)\n",
        "#         self.ignore_label = IGNORE_INDEX\n",
        "#         self.balance_weights = balance_weights\n",
        "#         self.sb_weights = sb_weights\n",
        "#         self.criterion = nn.CrossEntropyLoss(\n",
        "#             weight=weight,\n",
        "#             ignore_index=self.ignore_label,\n",
        "#             reduction='none'\n",
        "#         )\n",
        "\n",
        "#     def _ce_forward(self, score, target):\n",
        "#         loss = self.criterion(score, target)\n",
        "#         return loss\n",
        "\n",
        "#     def _ohem_forward(self, score, target, **kwargs):\n",
        "#         pred = F.softmax(score, dim=1)\n",
        "#         pixel_losses = self.criterion(score, target).contiguous().view(-1)\n",
        "#         mask = target.contiguous().view(-1) != self.ignore_label\n",
        "\n",
        "#         tmp_target = target.clone()\n",
        "#         tmp_target[tmp_target == self.ignore_label] = 0\n",
        "#         pred = pred.gather(1, tmp_target.unsqueeze(1))\n",
        "#         pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n",
        "#         min_value = pred[min(self.min_kept, pred.numel() - 1)]\n",
        "#         threshold = max(min_value, self.thresh)\n",
        "\n",
        "#         pixel_losses = pixel_losses[mask][ind]\n",
        "#         pixel_losses = pixel_losses[pred < threshold]\n",
        "#         return pixel_losses.mean()\n",
        "\n",
        "#     def forward(self, score, target):\n",
        "#         if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "#             score = [score]\n",
        "\n",
        "#         if len(self.balance_weights) == len(score):\n",
        "#             functions = [self._ce_forward] * \\\n",
        "#                 (len(self.balance_weights) - 1) + [self._ohem_forward]\n",
        "#             return sum([\n",
        "#                 w * func(x, target)\n",
        "#                 for (w, x, func) in zip(self.balance_weights, score, functions)\n",
        "#             ])\n",
        "\n",
        "#         elif len(score) == 1:\n",
        "#             return self.sb_weights * self._ohem_forward(score[0], target)\n",
        "\n",
        "#         else:\n",
        "#             raise ValueError(\"lengths of prediction and target are not identical!\")\n",
        "\n",
        "\n",
        "\n",
        "# LOSS BISENET\n",
        "class OhemCELoss(nn.Module):\n",
        "\n",
        "    def __init__(self, thresh, lb_ignore=255):\n",
        "        super(OhemCELoss, self).__init__()\n",
        "        self.thresh = -torch.log(torch.tensor(thresh, requires_grad=False, dtype=torch.float)).cuda()\n",
        "        self.lb_ignore = lb_ignore\n",
        "        self.criteria = nn.CrossEntropyLoss(ignore_index=lb_ignore, reduction='none')\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        n_min = labels[labels != self.lb_ignore].numel() // 16\n",
        "        loss = self.criteria(logits, labels).view(-1)\n",
        "        loss_hard = loss[loss > self.thresh]\n",
        "        if loss_hard.numel() < n_min:\n",
        "            loss_hard, _ = loss.topk(n_min)\n",
        "        return torch.mean(loss_hard)\n"
      ],
      "metadata": {
        "id": "7uJvTyhWp2Ky"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss2dPixelWiseWeighted(nn.Module):\n",
        "    def __init__(self, weight=None, ignore_index=250, reduction='none'):\n",
        "        super(CrossEntropyLoss2dPixelWiseWeighted, self).__init__()\n",
        "        self.CE =  nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index, reduction=reduction)\n",
        "\n",
        "    def forward(self, output, target, pixelWiseWeight):\n",
        "        loss = self.CE(output, target)\n",
        "        loss = torch.mean(loss * pixelWiseWeight)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "roNnNZCpmxls"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upscaling function"
      ],
      "metadata": {
        "id": "tTTJR3Ly3T_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def Upscaling(outputs, boundary_mask, model):\n",
        "    \"\"\"Upscale trough bilinear interpolation -> riporto le dimensioni dell'output a quelli originali\n",
        "    Quindi passiamo da 64 x 64 della rete a 512 x 512\"\"\"\n",
        "\n",
        "    h, w = boundary_mask.size(1), boundary_mask.size(2)\n",
        "    ph, pw = outputs[0].size(2), outputs[0].size(3)\n",
        "    if ph != h or pw != w:\n",
        "        for i in range(len(outputs)):\n",
        "            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n",
        "    if model.augment:\n",
        "        pred_p, pred_main, boundary_head = outputs  # P, I, D branches\n",
        "    else:\n",
        "        pred_p = None\n",
        "        pred_main = outputs\n",
        "        boundary_head = None  # Nessuna branch D se augment=False\n",
        "\n",
        "    return pred_p, pred_main, boundary_head"
      ],
      "metadata": {
        "id": "6A2YoQKT3Tu8"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate discriminator, optimizers and schedulers"
      ],
      "metadata": {
        "id": "B8_NYLc0kqRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bAbmz5Cz4I4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e714810c-96bf-4f75-fe88-59e4df7e54ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "86\n",
            "73\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import LambdaLR, SequentialLR, StepLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=GAMMA, patience=3, threshold=0.01)\n",
        "\n",
        "if TYPE_WEIGHT == \"median-frequency\" or TYPE_WEIGHT == \"inverse\" or TYPE_WEIGHT == \"log\":\n",
        "  loss_fn = OhemCELoss(thresh=0.8, lb_ignore = IGNORE_INDEX)\n",
        "  mix_loss = CrossEntropyLoss2dPixelWiseWeighted(ignore_index = IGNORE_INDEX, weight = torch.tensor(class_weights).cuda())\n",
        "else:\n",
        "  loss_fn = OhemCELoss(thresh=0.8, lb_ignore = IGNORE_INDEX)\n",
        "  mix_loss = CrossEntropyLoss2dPixelWiseWeighted(ignore_index = IGNORE_INDEX, weight = None)\n",
        "\n",
        "print(device)\n",
        "\n",
        "print(len(target_loader))\n",
        "print(len(source_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definition ema model"
      ],
      "metadata": {
        "id": "PBJn-0ZMR-WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ema_model(model):\n",
        "    \"\"\"Returns a new model that is used to generate pseudo-labels\"\"\"\n",
        "\n",
        "    ema_model = BiSeNetV2(NUM_CLASSES)\n",
        "\n",
        "    for param in ema_model.parameters():\n",
        "        param.detach_()\n",
        "    mp = list(model.parameters())\n",
        "    mcp = list(ema_model.parameters())\n",
        "    n = len(mp)\n",
        "    for i in range(0, n):\n",
        "        mcp[i].data[:] = mp[i].data[:].clone()\n",
        "\n",
        "    return ema_model\n",
        "\n",
        "\n",
        "def update_ema_variables(ema_model, model, alpha_teacher, iteration):\n",
        "    # Use the \"true\" average until the exponential average is more correct\n",
        "    alpha_teacher = min(1 - 1 / (iteration + 1), alpha_teacher)\n",
        "\n",
        "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "        #ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
        "        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n",
        "    return ema_model"
      ],
      "metadata": {
        "id": "kxmHtcIQR3N0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_class_mask(pred, classes):\n",
        "    pred, classes = torch.broadcast_tensors(pred.unsqueeze(0), classes.unsqueeze(1).unsqueeze(2))\n",
        "    N = pred.eq(classes).sum(0)\n",
        "    return N"
      ],
      "metadata": {
        "id": "E0WePh8KaIUS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oneMix(mask, data = None, target = None):\n",
        "    #Mix\n",
        "    if not (data is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], data[0])\n",
        "        data = (stackedMask0*data[0]+(1-stackedMask0)*data[1]).unsqueeze(0)\n",
        "    if not (target is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], target[0])\n",
        "        target = (stackedMask0*target[0]+(1-stackedMask0)*target[1]).unsqueeze(0)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "kZ1q-SXrczgq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strong_transform(target, masks_mix, aug = None, data = None):\n",
        "    data, target = oneMix(masks_mix, data, target)\n",
        "\n",
        "    if data is not None:\n",
        "      data_np = data.squeeze(0).cpu().numpy()\n",
        "    target_np = target.squeeze(0).cpu().numpy()\n",
        "\n",
        "    if data is not None:\n",
        "      data_np = np.transpose(data_np, (1, 2, 0))\n",
        "    target_np = np.transpose(target_np, (1, 2, 0))\n",
        "\n",
        "    if data is not None:\n",
        "      augmented = aug(image=data_np, mask=target_np)\n",
        "\n",
        "      data = augmented['image']\n",
        "      target = augmented['mask']\n",
        "\n",
        "      data = torch.from_numpy(data).permute(2, 0, 1).unsqueeze(0)  # (1, C, H, W)\n",
        "      target = torch.from_numpy(target).squeeze(-1).unsqueeze(0)  # (1, H, W)\n",
        "    else:\n",
        "      target = torch.from_numpy(target_np).squeeze(-1).unsqueeze(0)  # (1, H, W)\n",
        "      return None, target\n",
        "\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "KiJJDkIFcOpf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "jOCaZndAkPy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "kjNWfPicTLQ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "outputId": "bbe05d91-7da8-4f82-a557-d5c5a60e949f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights...\n",
            "Loaded weights!!\n",
            "[0.0003]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Training]: 100%|██████████| 73/73 [01:14<00:00,  1.02s/it, Loss_seg=92.9859]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 5.8116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1 [Validation]: 100%|██████████| 62/62 [00:14<00:00,  4.30it/s, Val_Loss=7.9528, mIoU=0.1438]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.4971\n",
            "→ Overall mIoU: 0.1438\n",
            "  → background IoU: 0.0376\n",
            "  → building IoU: 0.1583\n",
            "  → road IoU: 0.1984\n",
            "  → water IoU: 0.2439\n",
            "  → barren IoU: 0.1516\n",
            "  → forest IoU: 0.0308\n",
            "  → agriculture IoU: 0.1861\n",
            "[0.0003]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Training]:  11%|█         | 8/73 [00:10<01:23,  1.28s/it, Loss_seg=54.5909]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 189.38 MiB is free. Process 24562 has 21.97 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 399.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-a24976b0929f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mloss_overall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_seg_mix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_labeled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mloss_overall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 189.38 MiB is free. Process 24562 has 21.97 GiB memory in use. Of the allocated memory 21.36 GiB is allocated by PyTorch, and 399.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torchmetrics.segmentation import MeanIoU\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "ema_model = create_ema_model(model)\n",
        "ema_model.to(device)\n",
        "\n",
        "num_classes = 7\n",
        "miou_classes = MeanIoU(num_classes=num_classes, input_format = \"index\", per_class=True).to(device)\n",
        "\n",
        "num_aux_heads = 4\n",
        "criteria_aux = [OhemCELoss(0.8, IGNORE_INDEX) for _ in range(num_aux_heads)]\n",
        "\n",
        "criteria_aux_mix = [CrossEntropyLoss2dPixelWiseWeighted(ignore_index = IGNORE_INDEX, weight = torch.tensor(class_weights).cuda()) for _ in range(num_aux_heads)]\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(scheduler.get_last_lr())\n",
        "    loss_raw_value = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    model.train()\n",
        "    ema_model.train()\n",
        "\n",
        "    train_loader = zip(source_loader, target_loader)\n",
        "    num_batches = min(len(source_loader), len(target_loader))\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=num_batches, desc=f\"Epoch {epoch+1} [Training]\")\n",
        "\n",
        "    for i, (source_batch, target_batch) in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        X_source, y_source, _ = source_batch\n",
        "        X_source, y_source = X_source.to(device), y_source.to(device)\n",
        "\n",
        "        X_target, _ = target_batch\n",
        "        X_target = X_target.to(device)\n",
        "\n",
        "        outputs_s = model(X_source)\n",
        "\n",
        "        logits_labeled, *logits_aux_labeled  = outputs_s\n",
        "\n",
        "        loss_pre_labled = loss_fn(logits_labeled, y_source)\n",
        "        loss_aux_labeled = [crit(lgt, y_source) for crit, lgt in zip(criteria_aux, logits_aux_labeled)]\n",
        "        loss_labeled = loss_pre_labled + sum(loss_aux_labeled)\n",
        "\n",
        "        outputs_t = ema_model(X_target)\n",
        "\n",
        "        logits_unlabeled, *logits_aux_unlabeled  = outputs_t\n",
        "\n",
        "        probs_t = torch.softmax(logits_unlabeled.detach(), dim=1)\n",
        "\n",
        "        max_probs, pseudo_labels = torch.max(probs_t, dim=1)\n",
        "\n",
        "        MixMasks = []\n",
        "\n",
        "        for image_i in range(X_source.shape[0]): # Per ogni immagine source stiamo andando ad estrarre delle classi prese in modo random\n",
        "          classes = torch.unique(y_source[image_i])\n",
        "          classes = classes[classes!=-1]\n",
        "          nclasses = classes.shape[0]\n",
        "\n",
        "          classes = (classes[torch.Tensor(np.random.choice(nclasses, int((nclasses+nclasses%2)/2),replace=False)).long()]).cuda()\n",
        "\n",
        "          MixMask = generate_class_mask(y_source[image_i], classes).unsqueeze(0).cuda()\n",
        "          MixMasks.append(MixMask)\n",
        "\n",
        "        mixed_imgs = []\n",
        "        mixed_labels = []\n",
        "        mixed_boundary_masks = []\n",
        "\n",
        "        for image_i in range(X_source.shape[0]):\n",
        "          data = torch.cat((X_source[image_i].unsqueeze(0), X_target[image_i].unsqueeze(0)))\n",
        "          labels = torch.cat((y_source[image_i].unsqueeze(0), pseudo_labels[image_i].unsqueeze(0)))\n",
        "\n",
        "          data, mask = strong_transform(\n",
        "              aug=augment,\n",
        "              data=data,\n",
        "              target=labels,\n",
        "              masks_mix=[MixMasks[image_i]]\n",
        "          )\n",
        "\n",
        "          mixed_imgs.append(data)\n",
        "          mixed_labels.append(mask)\n",
        "\n",
        "        # if i == 0:\n",
        "        #   fig, axs = plt.subplots(2, 3, figsize=(24, 10))\n",
        "\n",
        "        #   for j in range(2):\n",
        "        #     axs[j, 0].imshow(mixed_imgs[j].squeeze().permute(1, 2, 0).cpu().detach().numpy())\n",
        "        #     axs[j, 0].set_title(\"IMG\")\n",
        "        #     axs[j, 0].axis('off')\n",
        "\n",
        "        #     axs[j, 1].imshow(mixed_labels[j].permute(1, 2, 0).cpu().detach().numpy())\n",
        "        #     axs[j, 1].set_title(\"Labels\")\n",
        "        #     axs[j, 1].axis('off')\n",
        "\n",
        "        #     axs[j, 2].imshow(mixed_boundary_masks[j].permute(1, 2, 0).cpu().detach().numpy())\n",
        "        #     axs[j, 2].set_title(\"Boundary\")\n",
        "        #     axs[j, 2].axis('off')\n",
        "\n",
        "        #   plt.tight_layout()\n",
        "        #   plt.show()\n",
        "\n",
        "\n",
        "        #Mi ricostruisce il batch completo da una lista di singole immagini e label mescolate (quindi con shape [1, C, H, W]) in [BATCH_SIZE, C, H, W]\n",
        "        inputs_mix = torch.cat(mixed_imgs, dim=0).to(device)\n",
        "        targets_mix = torch.cat(mixed_labels, dim=0).to(device)\n",
        "\n",
        "        outputs_mix = model(inputs_mix)\n",
        "\n",
        "        logits_mix, *logits_aux_mix = outputs_mix\n",
        "\n",
        "        #---------------------------------\n",
        "        if PIXEL_WEIGHT == \"threshold_uniform\":\n",
        "          unlabeled_weight = torch.sum(max_probs.ge(THRESHOLD).long() == 1).item() / np.size(np.array(targets_mix.cpu()))\n",
        "          pixelWiseWeight = unlabeled_weight * torch.ones(max_probs.shape).cuda()\n",
        "        else:\n",
        "          pixelWiseWeight = max_probs.ge(THRESHOLD).float().cuda()\n",
        "\n",
        "        onesWeights = torch.ones((pixelWiseWeight.shape)).cuda()\n",
        "\n",
        "        pixelWiseWeight_mix = []\n",
        "\n",
        "        for image_i in range(X_source.shape[0]):\n",
        "            weights_pair = torch.cat((onesWeights[image_i].unsqueeze(0), pixelWiseWeight[image_i].unsqueeze(0)))\n",
        "\n",
        "            _, mixed_weights = strong_transform(\n",
        "                target=weights_pair,\n",
        "                masks_mix=[MixMasks[image_i]]\n",
        "            )\n",
        "\n",
        "            pixelWiseWeight_mix.append(mixed_weights)\n",
        "\n",
        "        pixelWiseWeight_mix = torch.cat(pixelWiseWeight_mix, dim=0).to(device)  # [B, H, W]\n",
        "\n",
        "        loss_pre_mix = mix_loss(logits_mix, targets_mix, pixelWiseWeight_mix)\n",
        "        loss_aux_mix = [crit(lgt, y_source, pixelWiseWeight_mix) for crit, lgt in zip(criteria_aux_mix, logits_aux_mix)]\n",
        "        loss_seg_mix = loss_pre_mix + sum(loss_aux_labeled)\n",
        "\n",
        "\n",
        "        #---------------------------------------\n",
        "\n",
        "        loss_overall = loss_seg_mix + loss_labeled\n",
        "\n",
        "        loss_overall.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ema_model = update_ema_variables(ema_model = ema_model, model = model, alpha_teacher=ALPHA_TEACHER, iteration=(epoch * num_batches + i))\n",
        "\n",
        "        loss_raw_value += loss_overall.item()\n",
        "        total_train_samples += X_target.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            \"Loss_seg\": f\"{loss_raw_value / (i+1):.4f}\",\n",
        "        })\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary\")\n",
        "    print(f\"  → Segmentation Source Loss (RAW) : {loss_raw_value / total_train_samples:.4f}\")\n",
        "\n",
        "    # ---------------------- VALIDATION ----------------------\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    miou_classes.reset()\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Epoch {epoch+1} [Validation]\")\n",
        "\n",
        "        for batch, (X_val, y_val, boundary_mask) in pbar_val:\n",
        "            X_val, y_val, boundary_mask = X_val.to(device), y_val.to(device), boundary_mask.to(device)\n",
        "\n",
        "            outputs = model(X_val)\n",
        "            logits, *_ = outputs\n",
        "\n",
        "            loss = loss_fn(logits, y_val)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            total_val_samples += X_val.size(0)\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            valid_mask = (y_val >= 0) & (y_val < num_classes)\n",
        "            preds_flat = preds[valid_mask]\n",
        "            targets_flat = y_val[valid_mask]\n",
        "\n",
        "            miou_classes.update(preds_flat, targets_flat)\n",
        "\n",
        "            pbar_val.set_postfix({\n",
        "                \"Val_Loss\": f\"{val_loss / (batch+1):.4f}\",\n",
        "                \"mIoU\": f\"{miou_classes.compute().mean():.4f}\"\n",
        "            })\n",
        "\n",
        "    avg_val_loss = val_loss / total_val_samples\n",
        "    miou_per_class = miou_classes.compute()\n",
        "    miou = miou_per_class.mean()\n",
        "\n",
        "    print(f\"\\n→ Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"→ Overall mIoU: {miou:.4f}\")\n",
        "    for i, iou in enumerate(miou_per_class):\n",
        "        class_name = list(sem_class_to_idx.keys())[list(sem_class_to_idx.values()).index(i)]\n",
        "        print(f\"  → {class_name} IoU: {iou:.4f}\")\n",
        "\n",
        "    scheduler.step(miou)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ****************************** Validazione ******************************\n",
        "from torchmetrics.segmentation import MeanIoU\n",
        "\n",
        "num_classes = 7\n",
        "miou_classes = MeanIoU(num_classes=num_classes, input_format = \"index\", per_class=True).to(device)\n",
        "miou_overall = MeanIoU(num_classes=num_classes).to(device)\n",
        "\n",
        "model.eval()\n",
        "val_loss = 0\n",
        "miou_classes.reset()\n",
        "miou_overall.reset()\n",
        "total_val_samples = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch, (X_val, y_val, boundary_mask) in enumerate(val_loader):\n",
        "        X_val = X_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        boundary_mask = boundary_mask.to(device)\n",
        "\n",
        "        # Output del modello\n",
        "        outputs = model(X_val)\n",
        "\n",
        "        #upscaling\n",
        "        pred_p, pred_main, boundary_head = Upscaling(outputs=outputs, boundary_mask=boundary_mask, model=model)\n",
        "\n",
        "        if batch == 0:\n",
        "          fig, axs = plt.subplots(4, 5, figsize=(12, 5))\n",
        "\n",
        "          for j in range(4):\n",
        "\n",
        "            axs[j, 0].imshow(pred_p[j].cpu().detach().argmax(dim=0).numpy(), cmap='tab20')\n",
        "            axs[j, 0].set_title(\"Auxiliary Prediction\")\n",
        "            axs[j, 0].axis('off')\n",
        "\n",
        "            axs[j, 1].imshow(pred_main[j].cpu().detach().argmax(dim=0).numpy(), cmap='tab20')\n",
        "            axs[j, 1].set_title(\"Main Prediction\")\n",
        "            axs[j, 1].axis('off')\n",
        "\n",
        "            axs[j, 2].imshow(y_val[j].cpu().detach().numpy(), cmap='tab20')\n",
        "            axs[j, 2].set_title(\"Target mask\")\n",
        "            axs[j, 2].axis('off')\n",
        "\n",
        "            axs[j, 3].imshow(boundary_head[j].cpu().detach().sigmoid().squeeze(0).numpy(), cmap='gray')\n",
        "            axs[j, 3].set_title(\"Boundary Prediction\")\n",
        "            axs[j, 3].axis('off')\n",
        "\n",
        "            axs[j, 4].imshow(X_val[j].cpu().detach().squeeze(0).numpy().transpose(1, 2, 0))\n",
        "            axs[j, 4].set_title(\"Target image\")\n",
        "            axs[j, 4].axis('off')\n",
        "\n",
        "          plt.tight_layout()\n",
        "          plt.show()\n",
        "\n",
        "        # Calcola la loss\n",
        "        loss = loss_fn(pred_p, pred_main, y_val, boundary_head, boundary_mask)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        total_val_samples += X_val.size(0)\n",
        "\n",
        "        # Calcola le predizioni\n",
        "        preds = pred_main.argmax(dim=1)  # Shape: (N, H, W)\n",
        "\n",
        "        # Mask dei pixel validi (classi da 0 a num_classes - 1)\n",
        "        valid_mask = (y_val >= 0) & (y_val < num_classes)\n",
        "\n",
        "        # print(f\"valid mask :\", valid_mask.shape )\n",
        "\n",
        "        # Appiattisci le predizioni e i target solo sui pixel validi\n",
        "        preds_flat = preds[valid_mask]\n",
        "        targets_flat = y_val[valid_mask]\n",
        "\n",
        "        # print(f\"preds_flat :\", preds_flat.shape )\n",
        "        # print(f\"targets_flat :\", targets_flat.shape )\n",
        "\n",
        "        miou_classes.update(preds_flat, targets_flat)\n",
        "        miou_overall.update(preds_flat, targets_flat)\n",
        "\n",
        "avg_val_loss = val_loss / total_val_samples\n",
        "\n",
        "miou_per_class = miou_classes.compute()\n",
        "miou = miou_overall.compute()\n",
        "\n",
        "print(miou_per_class.mean())\n",
        "\n",
        "print(f\"Epoch {epoch+1}/{EPOCHS} | Validation Loss: {avg_val_loss} | mIoU: {miou} | Total validation samples seen: {total_val_samples}\")\n",
        "# per class\n",
        "for i, iou in enumerate(miou_per_class):\n",
        "    class_name = list(sem_class_to_idx.keys())[list(sem_class_to_idx.values()).index(i)]\n",
        "    print(f\"  → {class_name} IoU: {iou:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YPXuMPCoYmBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnrvRXfotSBq"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def load_inference_pidnet_model(id: int, name: str):\n",
        "    \"\"\"Returns the inference PIDNet model with `id`, it has to be saved in `content/drive/MyDrive/AML_project/checkpoints/`.\"\"\"\n",
        "\n",
        "    model = get_pred_model(name=name, num_classes=7)\n",
        "    try:\n",
        "        saved_weights = torch.load(\n",
        "            f\"content/drive/MyDrive/AML_project/checkpoints/model_PIDNET_3B_{id}.pth\",\n",
        "            map_location=torch.device(\"cpu\")\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        raise ValueError(\"This model was not found\")\n",
        "\n",
        "    model.load_state_dict(saved_weights)\n",
        "    return model"
      ],
      "metadata": {
        "id": "iI1BwvF-drOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvataggio del modello su drive"
      ],
      "metadata": {
        "id": "_rXI7eVx2TMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = f\"/content/drive/MyDrive/AML_project/checkpoints/model_PIDNET_0.23miou_4A.pth\"\n",
        "torch.save(model.state_dict(), checkpoint_path)\n",
        "print(f\"Modello salvato: {checkpoint_path}\")"
      ],
      "metadata": {
        "id": "P1ORzNPs1njC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRByJkGflhLX"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "import matplotlib.pyplot as plt\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "with torch.inference_mode():\n",
        "    # Prendi un'immagine random dal validation set\n",
        "    random_index = random.randint(0, len(val_dataset) - 1)\n",
        "    X, y, boundary_mask = val_dataset[random_index]\n",
        "\n",
        "    X = X.to(device).unsqueeze(dim=0)  # Aggiunge la dimensione batch\n",
        "    y = y.to(device).unsqueeze(dim=0)\n",
        "    boundary_mask = boundary_mask.to(device).unsqueeze(dim=0)\n",
        "\n",
        "    start = timer()\n",
        "    outputs = model(X)\n",
        "    end = timer()\n",
        "\n",
        "    latency = end - start\n",
        "\n",
        "    ## Upscale trough bilinear interpolation\n",
        "    h, w = boundary_mask.size(1), boundary_mask.size(2)\n",
        "    ph, pw = outputs[0].size(2), outputs[0].size(3)\n",
        "    print(f\"h: {h} | w: {w} | ph: {ph} | pw: {pw}\")\n",
        "    if ph != h or pw != w:\n",
        "        for i in range(len(outputs)):\n",
        "            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "    # Se augment=True, gestiamo i diversi output\n",
        "    if model.augment:\n",
        "        pred_p, pred_main, boundary_head = outputs\n",
        "    else:\n",
        "        pred_main = outputs\n",
        "        boundary_head = None\n",
        "\n",
        "    # Calcolo dei FLOPs\n",
        "    flops = FlopCountAnalysis(model, X.clone())\n",
        "\n",
        "    # Softmax per normalizzare le predizioni\n",
        "    normalized_masks = torch.nn.functional.softmax(pred_main, dim=1)\n",
        "\n",
        "    # Selezione delle predizioni per ciascuna classe\n",
        "    masks = [\n",
        "        normalized_masks[0, sem_class_to_idx[cls]]\n",
        "        for cls in SEM_CLASSES\n",
        "    ]\n",
        "\n",
        "    print(f\"FLOPs: {flops.total() / 1e9:.3f} GFLOPs\")\n",
        "    print(f\"Average inference latency is {latency:.3f} seconds.\")\n",
        "\n",
        "    # Converti output e ground truth per la visualizzazione\n",
        "    out = pred_main.squeeze().argmax(dim=0).cpu().numpy()  # Output segmentazione\n",
        "    y_np = y.squeeze().cpu().numpy()  # Ground truth\n",
        "    X_np = X.squeeze().cpu().permute(1, 2, 0).numpy()  # Immagine originale\n",
        "\n",
        "    # Creazione della figura con tre immagini affiancate\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "    # Immagine originale\n",
        "    axes[0].imshow(X_np)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    # Predizione del modello\n",
        "    axes[1].imshow(out, cmap=\"gray\")\n",
        "    axes[1].set_title(\"Predicted Segmentation\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    # Ground truth\n",
        "    axes[2].imshow(y_np, cmap=\"gray\")\n",
        "    axes[2].set_title(\"Ground Truth\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Conta i parametri del modello\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Params: {params / 1e6:.3f} M\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "UrjECeMs7Sc5",
        "aRC4KXtmj3Pi",
        "p_wwWFwFkIoR",
        "tTTJR3Ly3T_F"
      ],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}