{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngeloBongiorno/AML_2025_project4/blob/delivery/STEP_5_DACS_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7QJviwzoN5m"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHpwU5g8s-Pt"
      },
      "source": [
        "## Upload .zip files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2_aVBiH46K3"
      },
      "source": [
        "For this step you must have the zip files in your Drive into a folder called `AML_project`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xxVbeep6Rlnb",
        "outputId": "12a96f90-c25c-4ca4-ce5c-17c2dbe27cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.1\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore) (11.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=6a14f4ee880b8a50d50c8504d3ea9c3e699727c8fc4c11c60f3cb1ac015eb7b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=cde99b395bb91793b888cea7a2e769ebacd942ff999acbb88389ee01036e2946\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.1.1 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics\n",
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2LFD5EkeGs3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24a5fc2-a927-4fd9-881f-ffdc0f4d1185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AML_2025_project4'...\n",
            "remote: Enumerating objects: 211, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 211 (delta 56), reused 40 (delta 40), pack-reused 139 (from 2)\u001b[K\n",
            "Receiving objects: 100% (211/211), 94.53 MiB | 22.46 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "!git clone -b vito --single-branch https://github.com/AngeloBongiorno/AML_2025_project4.git\n",
        "!cp AML_2025_project4/utils.py .\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvdkrFwFI0Qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b51984-7411-436c-a6d6-bd1753b35a61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'utils' from '/content/utils.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import importlib\n",
        "import utils  # Replace with the actual module name\n",
        "\n",
        "importlib.reload(utils)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laEb8KOytCpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40d9037-d330-4b24-ab8f-ff43a34851c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting training...\n",
            "training extracted!\n",
            "Extracting validation...\n",
            "validation extracted!\n",
            "Extraction check completed!\n",
            "{'training_urban': '/content/dataset/Train/Urban', 'training_rural': '/content/dataset/Train/Rural', 'validation_urban': '/content/dataset/Val/Urban', 'validation_rural': '/content/dataset/Val/Rural'}\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "from utils import get_loveDA\n",
        "\n",
        "paths = get_loveDA(verbose=True)\n",
        "print(paths)\n",
        "\n",
        "TRAINING_PATH_URBAN = paths[\"training_urban\"]\n",
        "TRAINING_PATH_RURAL = paths[\"training_rural\"]\n",
        "VAL_PATH_URBAN = paths[\"validation_urban\"]\n",
        "VAL_PATH_RURAL = paths[\"validation_rural\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEM_CLASSES = [\n",
        "    'background',\n",
        "    'building',\n",
        "    'road',\n",
        "    'water',\n",
        "    'barren',\n",
        "    'forest',\n",
        "    'agriculture'\n",
        "]\n",
        "\n",
        "NUM_CLASSES = len(SEM_CLASSES)\n",
        "\n",
        "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(SEM_CLASSES)}\n",
        "\n",
        "IGNORE_INDEX = -1\n",
        "\n",
        "RESIZE = 512\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "EPOCHS = 20\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "STEP_SIZE = 21\n",
        "\n",
        "GAMMA = 0.5\n",
        "\n",
        "LR = 0.000329658544839708\n",
        "#LR = 0.00098\n",
        "\n",
        "P = 0.5 # probabilità augmentation\n",
        "\n",
        "ALPHA_TEACHER = 0.99\n",
        "\n",
        "THRESHOLD = 0.9\n",
        "\n",
        "MOMENTUM = 0.85\n",
        "\n",
        "LOSS_TYPE = \"ce\" # \"ohem\", \"ce\"\n",
        "\n",
        "TYPE_WEIGHT = \"inverse\" # median-frequency | inverse | log\n",
        "\n",
        "PIXEL_WEIGHT = \"threshold_uniform\" # \"threshold_uniform\", \"threshold\"\n",
        "\n",
        "SHOW_IMG = False\n"
      ],
      "metadata": {
        "id": "VJdiPeF5idkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and instantiate"
      ],
      "metadata": {
        "id": "dAYUGwGYiFGi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrjECeMs7Sc5"
      },
      "source": [
        "### Define PIDnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWTXrB6FZo_G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.no_relu = no_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "\n",
        "        if self.no_relu:\n",
        "            return out\n",
        "        else:\n",
        "            return self.relu(out)\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, no_relu=True):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(planes, momentum=bn_mom)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.bn3 = BatchNorm2d(planes * self.expansion, momentum=bn_mom)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.no_relu = no_relu\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        if self.no_relu:\n",
        "            return out\n",
        "        else:\n",
        "            return self.relu(out)\n",
        "\n",
        "class segmenthead(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes, interplanes, outplanes, scale_factor=None):\n",
        "        super(segmenthead, self).__init__()\n",
        "        self.bn1 = BatchNorm2d(inplanes, momentum=bn_mom)\n",
        "        self.conv1 = nn.Conv2d(inplanes, interplanes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = BatchNorm2d(interplanes, momentum=bn_mom)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(interplanes, outplanes, kernel_size=1, padding=0, bias=True)\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(self.relu(self.bn1(x)))\n",
        "        out = self.conv2(self.relu(self.bn2(x)))\n",
        "\n",
        "        if self.scale_factor is not None:\n",
        "            height = x.shape[-2] * self.scale_factor\n",
        "            width = x.shape[-1] * self.scale_factor\n",
        "            out = F.interpolate(out,\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        return out\n",
        "\n",
        "class DAPPM(nn.Module):\n",
        "    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n",
        "        super(DAPPM, self).__init__()\n",
        "        bn_mom = 0.1\n",
        "        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale0 = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.process1 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.process2 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.process3 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.process4 = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes, branch_planes, kernel_size=3, padding=1, bias=False),\n",
        "                                    )\n",
        "        self.compression = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.shortcut = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        width = x.shape[-1]\n",
        "        height = x.shape[-2]\n",
        "        x_list = []\n",
        "\n",
        "        x_list.append(self.scale0(x))\n",
        "        x_list.append(self.process1((F.interpolate(self.scale1(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[0])))\n",
        "        x_list.append((self.process2((F.interpolate(self.scale2(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[1]))))\n",
        "        x_list.append(self.process3((F.interpolate(self.scale3(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[2])))\n",
        "        x_list.append(self.process4((F.interpolate(self.scale4(x),\n",
        "                        size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_list[3])))\n",
        "\n",
        "        out = self.compression(torch.cat(x_list, 1)) + self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "class PAPPM(nn.Module):\n",
        "    def __init__(self, inplanes, branch_planes, outplanes, BatchNorm=nn.BatchNorm2d):\n",
        "        super(PAPPM, self).__init__()\n",
        "        bn_mom = 0.1\n",
        "        self.scale1 = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=2, padding=2),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale2 = nn.Sequential(nn.AvgPool2d(kernel_size=9, stride=4, padding=4),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale3 = nn.Sequential(nn.AvgPool2d(kernel_size=17, stride=8, padding=8),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "        self.scale4 = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "        self.scale0 = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, branch_planes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "        self.scale_process = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes*4, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes*4, branch_planes*4, kernel_size=3, padding=1, groups=4, bias=False),\n",
        "                                    )\n",
        "\n",
        "\n",
        "        self.compression = nn.Sequential(\n",
        "                                    BatchNorm(branch_planes * 5, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(branch_planes * 5, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "        self.shortcut = nn.Sequential(\n",
        "                                    BatchNorm(inplanes, momentum=bn_mom),\n",
        "                                    nn.ReLU(inplace=True),\n",
        "                                    nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=False),\n",
        "                                    )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        width = x.shape[-1]\n",
        "        height = x.shape[-2]\n",
        "        scale_list = []\n",
        "\n",
        "        x_ = self.scale0(x)\n",
        "        scale_list.append(F.interpolate(self.scale1(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "        scale_list.append(F.interpolate(self.scale2(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "        scale_list.append(F.interpolate(self.scale3(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "        scale_list.append(F.interpolate(self.scale4(x), size=[height, width],\n",
        "                        mode='bilinear', align_corners=algc)+x_)\n",
        "\n",
        "        scale_out = self.scale_process(torch.cat(scale_list, 1))\n",
        "\n",
        "        out = self.compression(torch.cat([x_,scale_out], 1)) + self.shortcut(x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PagFM(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, after_relu=False, with_channel=False, BatchNorm=nn.BatchNorm2d):\n",
        "        super(PagFM, self).__init__()\n",
        "        self.with_channel = with_channel\n",
        "        self.after_relu = after_relu\n",
        "        self.f_x = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, mid_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(mid_channels)\n",
        "                                )\n",
        "        self.f_y = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, mid_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(mid_channels)\n",
        "                                )\n",
        "        if with_channel:\n",
        "            self.up = nn.Sequential(\n",
        "                                    nn.Conv2d(mid_channels, in_channels,\n",
        "                                              kernel_size=1, bias=False),\n",
        "                                    BatchNorm(in_channels)\n",
        "                                   )\n",
        "        if after_relu:\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        input_size = x.size()\n",
        "        if self.after_relu:\n",
        "            y = self.relu(y)\n",
        "            x = self.relu(x)\n",
        "\n",
        "        y_q = self.f_y(y)\n",
        "        y_q = F.interpolate(y_q, size=[input_size[2], input_size[3]],\n",
        "                            mode='bilinear', align_corners=False)\n",
        "        x_k = self.f_x(x)\n",
        "\n",
        "        if self.with_channel:\n",
        "            sim_map = torch.sigmoid(self.up(x_k * y_q))\n",
        "        else:\n",
        "            sim_map = torch.sigmoid(torch.sum(x_k * y_q, dim=1).unsqueeze(1))\n",
        "\n",
        "        y = F.interpolate(y, size=[input_size[2], input_size[3]],\n",
        "                            mode='bilinear', align_corners=False)\n",
        "        x = (1-sim_map)*x + sim_map*y\n",
        "\n",
        "        return x\n",
        "\n",
        "class Light_Bag(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
        "        super(Light_Bag, self).__init__()\n",
        "        self.conv_p = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "        self.conv_i = nn.Sequential(\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "\n",
        "    def forward(self, p, i, d):\n",
        "        edge_att = torch.sigmoid(d)\n",
        "\n",
        "        p_add = self.conv_p((1-edge_att)*i + p)\n",
        "        i_add = self.conv_i(i + edge_att*p)\n",
        "\n",
        "        return p_add + i_add\n",
        "\n",
        "\n",
        "class DDFMv2(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
        "        super(DDFMv2, self).__init__()\n",
        "        self.conv_p = nn.Sequential(\n",
        "                                BatchNorm(in_channels),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "        self.conv_i = nn.Sequential(\n",
        "                                BatchNorm(in_channels),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=1, bias=False),\n",
        "                                BatchNorm(out_channels)\n",
        "                                )\n",
        "\n",
        "    def forward(self, p, i, d):\n",
        "        edge_att = torch.sigmoid(d)\n",
        "\n",
        "        p_add = self.conv_p((1-edge_att)*i + p)\n",
        "        i_add = self.conv_i(i + edge_att*p)\n",
        "\n",
        "        return p_add + i_add\n",
        "\n",
        "class Bag(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, BatchNorm=nn.BatchNorm2d):\n",
        "        super(Bag, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "                                BatchNorm(in_channels),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Conv2d(in_channels, out_channels,\n",
        "                                          kernel_size=3, padding=1, bias=False)\n",
        "                                )\n",
        "\n",
        "\n",
        "    def forward(self, p, i, d):\n",
        "        edge_att = torch.sigmoid(d)\n",
        "        return self.conv(edge_att*p + (1-edge_att)*i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjCDANDmZZw3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import logging\n",
        "\n",
        "BatchNorm2d = nn.BatchNorm2d\n",
        "bn_mom = 0.1\n",
        "algc = False\n",
        "\n",
        "\n",
        "\n",
        "class PIDNet(nn.Module):\n",
        "\n",
        "    def __init__(self, m=2, n=3, num_classes=19, planes=64, ppm_planes=96, head_planes=128, augment=True):\n",
        "        super(PIDNet, self).__init__()\n",
        "        self.augment = augment\n",
        "\n",
        "        # I Branch\n",
        "        self.conv1 =  nn.Sequential(\n",
        "                          nn.Conv2d(3,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                          nn.Conv2d(planes,planes,kernel_size=3, stride=2, padding=1),\n",
        "                          BatchNorm2d(planes, momentum=bn_mom),\n",
        "                          nn.ReLU(inplace=True),\n",
        "                      )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(BasicBlock, planes, planes, m)\n",
        "        self.layer2 = self._make_layer(BasicBlock, planes, planes * 2, m, stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, planes * 2, planes * 4, n, stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, planes * 4, planes * 8, n, stride=2)\n",
        "        self.layer5 =  self._make_layer(Bottleneck, planes * 8, planes * 8, 2, stride=2)\n",
        "\n",
        "        # P Branch\n",
        "        self.compression3 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 4, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "\n",
        "        self.compression4 = nn.Sequential(\n",
        "                                          nn.Conv2d(planes * 8, planes * 2, kernel_size=1, bias=False),\n",
        "                                          BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                          )\n",
        "        self.pag3 = PagFM(planes * 2, planes)\n",
        "        self.pag4 = PagFM(planes * 2, planes)\n",
        "\n",
        "        self.layer3_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer4_ = self._make_layer(BasicBlock, planes * 2, planes * 2, m)\n",
        "        self.layer5_ = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # D Branch\n",
        "        if m == 2:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes)\n",
        "            self.layer4_d = self._make_layer(Bottleneck, planes, planes, 1)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = PAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Light_Bag(planes * 4, planes * 4)\n",
        "        else:\n",
        "            self.layer3_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.layer4_d = self._make_single_layer(BasicBlock, planes * 2, planes * 2)\n",
        "            self.diff3 = nn.Sequential(\n",
        "                                        nn.Conv2d(planes * 4, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                        BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                        )\n",
        "            self.diff4 = nn.Sequential(\n",
        "                                     nn.Conv2d(planes * 8, planes * 2, kernel_size=3, padding=1, bias=False),\n",
        "                                     BatchNorm2d(planes * 2, momentum=bn_mom),\n",
        "                                     )\n",
        "            self.spp = DAPPM(planes * 16, ppm_planes, planes * 4)\n",
        "            self.dfm = Bag(planes * 4, planes * 4)\n",
        "\n",
        "        self.layer5_d = self._make_layer(Bottleneck, planes * 2, planes * 2, 1)\n",
        "\n",
        "        # Prediction Head\n",
        "        if self.augment:\n",
        "            self.seghead_p = segmenthead(planes * 2, head_planes, num_classes)\n",
        "            self.seghead_d = segmenthead(planes * 2, planes, 1)\n",
        "\n",
        "        self.final_layer = segmenthead(planes * 4, head_planes, num_classes)\n",
        "\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes, planes, stride, downsample))\n",
        "        inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            if i == (blocks-1):\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=True))\n",
        "            else:\n",
        "                layers.append(block(inplanes, planes, stride=1, no_relu=False))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_single_layer(self, block, inplanes, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, momentum=bn_mom),\n",
        "            )\n",
        "\n",
        "        layer = block(inplanes, planes, stride, downsample, no_relu=True)\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        width_output = x.shape[-1] // 8\n",
        "        height_output = x.shape[-2] // 8\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(self.layer2(self.relu(x)))\n",
        "        x_ = self.layer3_(x)\n",
        "        x_d = self.layer3_d(x)\n",
        "\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x_ = self.pag3(x_, self.compression3(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff3(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_p = x_\n",
        "\n",
        "        x = self.relu(self.layer4(x))\n",
        "        x_ = self.layer4_(self.relu(x_))\n",
        "        x_d = self.layer4_d(self.relu(x_d))\n",
        "\n",
        "        x_ = self.pag4(x_, self.compression4(x))\n",
        "        x_d = x_d + F.interpolate(\n",
        "                        self.diff4(x),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "        if self.augment:\n",
        "            temp_d = x_d\n",
        "\n",
        "        x_ = self.layer5_(self.relu(x_))\n",
        "        x_d = self.layer5_d(self.relu(x_d))\n",
        "        x = F.interpolate(\n",
        "                        self.spp(self.layer5(x)),\n",
        "                        size=[height_output, width_output],\n",
        "                        mode='bilinear', align_corners=algc)\n",
        "\n",
        "        x_ = self.final_layer(self.dfm(x_, x, x_d))\n",
        "\n",
        "        if self.augment:\n",
        "            x_extra_p = self.seghead_p(temp_p)\n",
        "            x_extra_d = self.seghead_d(temp_d)\n",
        "            return [x_extra_p, x_, x_extra_d]\n",
        "        else:\n",
        "            return x_\n",
        "\n",
        "def get_seg_model(cfg, imgnet_pretrained):\n",
        "\n",
        "    if 's' in cfg.MODEL.NAME:\n",
        "        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=32, ppm_planes=96, head_planes=128, augment=True)\n",
        "    elif 'm' in cfg.MODEL.NAME:\n",
        "        model = PIDNet(m=2, n=3, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=96, head_planes=128, augment=True)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=cfg.DATASET.NUM_CLASSES, planes=64, ppm_planes=112, head_planes=256, augment=True)\n",
        "\n",
        "    if imgnet_pretrained:\n",
        "        pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_state = {k: v for k, v in pretrained_state.items() if (k in model_dict and v.shape == model_dict[k].shape)}\n",
        "        model_dict.update(pretrained_state)\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_state))\n",
        "        logging.info('Attention!!!')\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "    else:\n",
        "        pretrained_dict = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')\n",
        "        if 'state_dict' in pretrained_dict:\n",
        "            pretrained_dict = pretrained_dict['state_dict']\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k[6:]: v for k, v in pretrained_dict.items() if (k[6:] in model_dict and v.shape == model_dict[k[6:]].shape)}\n",
        "        msg = 'Loaded {} parameters!'.format(len(pretrained_dict))\n",
        "        logging.info('Attention!!!')\n",
        "        logging.info(msg)\n",
        "        logging.info('Over!!!')\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict, strict = False)\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_pred_model(name, num_classes):\n",
        "\n",
        "    if 's' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=32, ppm_planes=96, head_planes=128, augment=False)\n",
        "    elif 'm' in name:\n",
        "        model = PIDNet(m=2, n=3, num_classes=num_classes, planes=64, ppm_planes=96, head_planes=128, augment=False)\n",
        "    else:\n",
        "        model = PIDNet(m=3, n=4, num_classes=num_classes, planes=64, ppm_planes=112, head_planes=256, augment=False)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw9SYUCgi6us"
      },
      "source": [
        "# Dataset & dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset definition"
      ],
      "metadata": {
        "id": "wrMzI_LbjhP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6kSW8hGjAo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd79f69-782d-4e45-a8e3-be5f832f40a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.8' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform, target=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
        "        self.target = target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
        "\n",
        "        # Read an image with OpenCV\n",
        "        image = cv2.imread(img_path)\n",
        "        mask = cv2.imread(mask_path)\n",
        "\n",
        "        # By default OpenCV uses BGR color space for color images,\n",
        "        # so we need to convert the image to RGB color space.\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed[\"image\"]\n",
        "            mask = transformed[\"mask\"]\n",
        "\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "        edge = cv2.Canny(mask_np, 0.1, 0.2)\n",
        "\n",
        "        kernel = np.ones((3, 3), np.uint8)  # Kernel for dilation\n",
        "\n",
        "        edge = edge[6:-6, 6:-6]\n",
        "        edge = np.pad(edge, ((6,6),(6,6)), mode='constant')\n",
        "        boundaries = cv2.dilate(edge, kernel, iterations=1)  # Dilate edges\n",
        "        boundaries = (boundaries > 50) * 1.0 # boundaries matrix is float with 1.0 or 0.0\n",
        "\n",
        "        mask = torch.as_tensor(np.array(mask), dtype=torch.int64) - 1\n",
        "\n",
        "        boundaries_tensor = torch.as_tensor(boundaries, dtype=torch.float32)\n",
        "\n",
        "        # if the dataset is a target dataset, does not return the mask\n",
        "        if self.target == True:\n",
        "          return image, boundaries_tensor\n",
        "        return image, mask, boundaries_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for images & masks\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torchvision.transforms import v2 as T\n",
        "import cv2\n",
        "\n",
        "resize_transform = A.Compose([\n",
        "    A.Resize(height=RESIZE, width=RESIZE, p=1),\n",
        "    A.ToFloat(),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# The best augmentation from previous step is chosen\n",
        "alb_aug0 = A.HorizontalFlip(p=P)\n",
        "alb_aug1 = A.GaussianBlur(p=P, sigma_limit=(0.5, 3.0))\n",
        "alb_aug4 = A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=P)\n",
        "\n",
        "augment = A.Compose([alb_aug4, alb_aug0, alb_aug1])"
      ],
      "metadata": {
        "id": "cOr2yJ_6kvA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_boundary_mask(mask):\n",
        "    # Converts the input mask to a NumPy array if it's a PyTorch tensor, ensuring the shape is [H, W].\n",
        "    if isinstance(mask, torch.Tensor):\n",
        "        mask_np = mask.squeeze().cpu().numpy()\n",
        "    else:\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "    # Converts the mask to unsigned 8-bit integers for OpenCV processing.\n",
        "    mask_np = mask_np.astype(np.uint8)\n",
        "\n",
        "    # Applies the Canny edge detector to find edges in the mask.\n",
        "    edge = cv2.Canny(mask_np, 0.1, 0.2)\n",
        "\n",
        "    # Defines a 3x3 kernel of ones for dilation.\n",
        "    kernel = np.ones((3, 3), np.uint8)\n",
        "\n",
        "    # Crops 6 pixels from each border of the edge map, then pads it back with zeros.\n",
        "    edge = edge[6:-6, 6:-6]\n",
        "    edge = np.pad(edge, ((6,6),(6,6)), mode='constant')\n",
        "\n",
        "    # Dilates the edge map to thicken boundary lines.\n",
        "    boundaries = cv2.dilate(edge, kernel, iterations=1)\n",
        "\n",
        "    # Converts the dilated boundaries to a float32 binary mask (values 0 or 1).\n",
        "    boundaries = (boundaries > 50).astype(np.float32)\n",
        "\n",
        "    # Converts the boundaries back to a PyTorch tensor with shape [1, H, W] for further processing.\n",
        "    boundaries_tensor = torch.from_numpy(boundaries).unsqueeze(0)\n",
        "    return boundaries_tensor"
      ],
      "metadata": {
        "id": "fbbVpaD-McpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset instantiation"
      ],
      "metadata": {
        "id": "aRC4KXtmj3Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects\n",
        "train_and_val_dataset_urban = SegmentationDataset(\n",
        "    TRAINING_PATH_URBAN + \"/images_png\",\n",
        "    TRAINING_PATH_URBAN + \"/masks_png\",\n",
        "    transform=resize_transform\n",
        ")\n",
        "\n",
        "val_ratio = 0.2\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "val_size = int(len(train_and_val_dataset_urban) * val_ratio)\n",
        "train_size = len(train_and_val_dataset_urban) - val_size\n",
        "\n",
        "source_dataset, val_dataset = random_split(train_and_val_dataset_urban, [train_size, val_size], generator=generator)\n",
        "print(f\"Source dataset size: {len(source_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")\n",
        "\n",
        "target_dataset = SegmentationDataset(TRAINING_PATH_RURAL + \"/images_png\", TRAINING_PATH_RURAL + \"/masks_png\",\n",
        "                                    transform=resize_transform, target=True)\n",
        "print(f\"Target dataset size: {len(target_dataset)}\")\n",
        "\n",
        "# TEST DATASET\n",
        "test_dataset = SegmentationDataset(VAL_PATH_RURAL + \"/images_png\", VAL_PATH_RURAL + \"/masks_png\",\n",
        "                                    transform=resize_transform)\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "Zk4ZifehjuyZ",
        "outputId": "c665474c-dc8e-49a7-c4d1-b3c972349dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source dataset size: 925\n",
            "Validation size: 231\n",
            "Target dataset size: 1366\n",
            "Test dataset size: 992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assume NUM_CLASSES and BATCH_SIZE are defined, and source_dataset is your dataset\n",
        "\n",
        "class_counts = torch.zeros(NUM_CLASSES)  # Initialize a tensor to count pixels per class\n",
        "\n",
        "# Convert the counts to numpy for easier numerical operations\n",
        "class_counts = class_counts.numpy()\n",
        "\n",
        "total_pixels = np.sum(class_counts)  # Total number of labeled pixels in the dataset\n",
        "\n",
        "frequencies = class_counts / total_pixels  # Relative frequency of each class (pixels_per_class / total_pixels)\n",
        "\n",
        "# Choose the type of class weighting to apply\n",
        "if TYPE_WEIGHT == \"inverse\":\n",
        "  # Inverse frequency weighting (rare classes get higher weight)\n",
        "  # Example precomputed values (dynamic calculation commented out)\n",
        "  # class_weights = 1.0 / (frequencies + 1e-8)\n",
        "  class_weights = [2.063954, 4.717028, 10.776062, 26.797655, 13.217381, 12.630906, 53.904175]\n",
        "\n",
        "elif TYPE_WEIGHT == \"median-frequency\":\n",
        "  # Median frequency balancing weighting\n",
        "  # median = np.median(frequencies)\n",
        "  # class_weights = median / (frequencies + 1e-8)\n",
        "  class_weights = [0.16340506, 0.37345123, 0.85315025, 2.1215937, 1.0464315, 0.9999999, 4.2676406]\n",
        "\n",
        "elif TYPE_WEIGHT == \"log\":\n",
        "  # Logarithmic smoothing weighting\n",
        "  # class_weights = 1.0 / np.log(1.02 + frequencies)\n",
        "  class_weights = [2.4481893, 4.793011, 9.3564825, 17.942291, 10.946302, 10.575735, 26.43621]\n",
        "\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-PvK8l4668q",
        "outputId": "334df0d4-bb9e-4f49-8b91-83ac65edbe88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.063954, 4.717028, 10.776062, 26.797655, 13.217381, 12.630906, 53.904175]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d3d8c6f7f018>:16: RuntimeWarning: invalid value encountered in divide\n",
            "  frequencies = class_counts / total_pixels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loader instantiation"
      ],
      "metadata": {
        "id": "lyTIKtzjj7i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders\n",
        "\n",
        "# TRAINING DATALOADERS\n",
        "source_loader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# TEST DATALOADER\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# enumerate dataloaders\n",
        "source_loader_iter = enumerate(source_loader)\n",
        "target_loader_iter = enumerate(target_loader)"
      ],
      "metadata": {
        "id": "vhsQeNzTj_mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4pcqQXrMzza"
      },
      "source": [
        "### Instantiate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZNWIM0DbnJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac841864-550a-4b2e-920e-2d97b3806a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\n",
            "To: /content/PIDNet_S_ImageNet.pth.tar\n",
            "100%|██████████| 38.1M/38.1M [00:00<00:00, 81.4MB/s]\n",
            "<ipython-input-7-a9464f4ede7a>:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "imagenet-pretrained pidnet weights downloaded\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "if (os.path.exists(\"./PIDNet_S_ImageNet.pth.tar\") == False):\n",
        "  url = \"https://drive.google.com/uc?id=1hIBp_8maRr60-B3PF0NVtaA6TYBvO4y-\"\n",
        "  output = \"./\"\n",
        "  gdown.download(url, output, quiet=False)\n",
        "\n",
        "  print(\"imagenet-pretrained pidnet weights downloaded\")\n",
        "\n",
        "\n",
        "class Config:\n",
        "  class MODEL:\n",
        "      NAME = 'pidnet_s'\n",
        "      PRETRAINED = 'PIDNet_S_ImageNet.pth.tar'\n",
        "  class DATASET:\n",
        "      NUM_CLASSES = NUM_CLASSES\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "model = get_seg_model(cfg, imgnet_pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSQUcy7_t2of"
      },
      "source": [
        "# Training Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define loss functions"
      ],
      "metadata": {
        "id": "p_wwWFwFkIoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Extra Semantic Loss (Classica CrossEntropy Loss)\n",
        "class CrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, num_outputs, weight=None, balance_weights=[0.4, 1.0], sb_weights=1.0):\n",
        "        super(CrossEntropyLoss, self).__init__()\n",
        "        self.loss = nn.CrossEntropyLoss(weight=weight, ignore_index=IGNORE_INDEX)\n",
        "        self.num_outputs = num_outputs\n",
        "        self.balance_weights = balance_weights\n",
        "        self.sb_weights = sb_weights\n",
        "\n",
        "    def _forward(self, pred, target):\n",
        "        return self.loss(pred, target)\n",
        "\n",
        "    def forward(self, score, target):\n",
        "        if self.num_outputs == 1:\n",
        "            score = [score]\n",
        "\n",
        "        if len(self.balance_weights) == len(score):\n",
        "            return sum([w * self._forward(x, target) for (w, x) in zip(self.balance_weights, score)])\n",
        "        elif len(score) == 1:\n",
        "            return self.sb_weights * self._forward(score[0], target)\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")\n",
        "\n",
        "class OhemCrossEntropy(nn.Module):\n",
        "    def __init__(self, thres=0.7, min_kept=26_000, balance_weights=[0.4, 1.0], sb_weights=1.0, weight=None):\n",
        "        super(OhemCrossEntropy, self).__init__()\n",
        "        self.thresh = thres\n",
        "        self.min_kept = max(1, min_kept)\n",
        "        self.ignore_label = IGNORE_INDEX\n",
        "        self.balance_weights = balance_weights\n",
        "        self.sb_weights = sb_weights\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=self.ignore_label,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "    def _ce_forward(self, score, target):\n",
        "        loss = self.criterion(score, target)\n",
        "        return loss\n",
        "\n",
        "    def _ohem_forward(self, score, target, **kwargs):\n",
        "        pred = F.softmax(score, dim=1)\n",
        "        pixel_losses = self.criterion(score, target).contiguous().view(-1)\n",
        "        mask = target.contiguous().view(-1) != self.ignore_label\n",
        "\n",
        "        tmp_target = target.clone()\n",
        "        tmp_target[tmp_target == self.ignore_label] = 0\n",
        "        pred = pred.gather(1, tmp_target.unsqueeze(1))\n",
        "        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n",
        "        min_value = pred[min(self.min_kept, pred.numel() - 1)]\n",
        "        threshold = max(min_value, self.thresh)\n",
        "\n",
        "        pixel_losses = pixel_losses[mask][ind]\n",
        "        pixel_losses = pixel_losses[pred < threshold]\n",
        "        return pixel_losses.mean()\n",
        "\n",
        "    def forward(self, score, target):\n",
        "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "            score = [score]\n",
        "\n",
        "        if len(self.balance_weights) == len(score):\n",
        "            functions = [self._ce_forward] * \\\n",
        "                (len(self.balance_weights) - 1) + [self._ohem_forward]\n",
        "            return sum([\n",
        "                w * func(x, target)\n",
        "                for (w, x, func) in zip(self.balance_weights, score, functions)\n",
        "            ])\n",
        "\n",
        "        elif len(score) == 1:\n",
        "            return self.sb_weights * self._ohem_forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")\n",
        "\n",
        "\n",
        "# Weighted Binary Cross Entropy per i bordi\n",
        "def weighted_bce(bd_pre, target):\n",
        "    n, c, h, w = bd_pre.size()\n",
        "    log_p = bd_pre.permute(0,2,3,1).contiguous().view(1, -1)\n",
        "    target_t = target.view(1, -1)\n",
        "\n",
        "    pos_index = (target_t == 1)\n",
        "    neg_index = (target_t == 0)\n",
        "\n",
        "    weight = torch.zeros_like(log_p)\n",
        "    pos_num = pos_index.sum()\n",
        "    neg_num = neg_index.sum()\n",
        "    sum_num = pos_num + neg_num\n",
        "    weight[pos_index] = neg_num * 1.0 / sum_num\n",
        "    weight[neg_index] = pos_num * 1.0 / sum_num\n",
        "\n",
        "    loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, reduction='mean')\n",
        "\n",
        "    return loss\n",
        "\n",
        "class BondaryLoss(nn.Module):\n",
        "    def __init__(self, coeff_bce = 20.0):\n",
        "        super(BondaryLoss, self).__init__()\n",
        "        self.coeff_bce = coeff_bce\n",
        "\n",
        "    def forward(self, bd_pre, bd_gt):\n",
        "        bce_loss = self.coeff_bce * weighted_bce(bd_pre, bd_gt)\n",
        "        loss = bce_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "# PIDNet Loss Totale\n",
        "class PIDNetLoss(nn.Module):\n",
        "    def __init__(self, lambda_0=0.4, lambda_1=20, lambda_2=1, lambda_3=1, threshold=0.8, class_weights=None):\n",
        "        super(PIDNetLoss, self).__init__()\n",
        "        self.class_weights = class_weights\n",
        "        if self.class_weights is not None:\n",
        "            self.class_weights = torch.tensor(class_weights).cuda()\n",
        "        if LOSS_TYPE == \"ohem\":\n",
        "          self.sem_loss = OhemCrossEntropy(balance_weights=[lambda_0, lambda_2], sb_weights=lambda_3, weight = self.class_weights)\n",
        "        else:\n",
        "          self.sem_loss = CrossEntropyLoss(num_outputs=2, balance_weights=[lambda_0, lambda_2], sb_weights=lambda_3, weight = self.class_weights)\n",
        "        self.bd_loss = BondaryLoss(coeff_bce=lambda_1)\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def forward(self, pred_p, pred_main, target, boundary_head, boundary_mask):\n",
        "        \"\"\"\n",
        "        pred_p: output branch P (B, C, H, W)\n",
        "        pred_main: output principale (B, C, H, W)\n",
        "        target: ground truth segmentazione (B, H, W)\n",
        "        boundary_head: predizione dei bordi (B, 1, H, W)\n",
        "        boundary_mask: ground truth dei bordi (B, 1, H, W)\n",
        "        \"\"\"\n",
        "\n",
        "        loss_s = self.sem_loss([pred_p, pred_main], target) # l_0 e l_2\n",
        "        loss_b = self.bd_loss(boundary_head, boundary_mask.unsqueeze(1)) # l_1\n",
        "\n",
        "        # l_3\n",
        "        filler = torch.ones_like(target) * IGNORE_INDEX\n",
        "        bd_label = torch.where(F.sigmoid(boundary_head[:,0,:,:])>self.threshold, target, filler)\n",
        "        loss_sb = self.sem_loss([pred_main], bd_label)\n",
        "\n",
        "\n",
        "        loss = loss_s + loss_b + loss_sb\n",
        "\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "7uJvTyhWp2Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss2dPixelWiseWeighted(nn.Module):\n",
        "    def __init__(self, weight=None, ignore_index=250, reduction='none'):\n",
        "        super(CrossEntropyLoss2dPixelWiseWeighted, self).__init__()\n",
        "        self.CE =  nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_index, reduction=reduction)\n",
        "\n",
        "    def forward(self, output, target, pixelWiseWeight):\n",
        "        loss = self.CE(output, target)\n",
        "        loss = torch.mean(loss * pixelWiseWeight)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "_saxZ8fwHpYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upscaling function"
      ],
      "metadata": {
        "id": "tTTJR3Ly3T_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def Upscaling(outputs, boundary_mask, model):\n",
        "    \"\"\"Upscale via bilinear interpolation — resize output to original dimensions\n",
        "    So we go from 64 x 64 network output to 512 x 512\"\"\"\n",
        "\n",
        "    h, w = boundary_mask.size(1), boundary_mask.size(2)  # original height and width\n",
        "    ph, pw = outputs[0].size(2), outputs[0].size(3)  # current output height and width\n",
        "\n",
        "    if ph != h or pw != w:  # if sizes differ, upscale all outputs\n",
        "        for i in range(len(outputs)):\n",
        "            outputs[i] = F.interpolate(outputs[i], size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "    if model.augment:  # if augmentation enabled, unpack outputs from all branches\n",
        "        pred_p, pred_main, boundary_head = outputs  # P, I, D branches\n",
        "    else:\n",
        "        pred_p = None\n",
        "        pred_main = outputs  # main output only\n",
        "        boundary_head = None  # no boundary branch when augment is False\n",
        "\n",
        "    return pred_p, pred_main, boundary_head"
      ],
      "metadata": {
        "id": "6A2YoQKT3Tu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate discriminator, optimizers and schedulers"
      ],
      "metadata": {
        "id": "B8_NYLc0kqRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAbmz5Cz4I4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d369a2bc-036a-40ef-c6f1-506f402196e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "86\n",
            "58\n"
          ]
        }
      ],
      "source": [
        "from torch.optim.lr_scheduler import LambdaLR, SequentialLR, StepLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=GAMMA, patience=3, threshold=0.01)\n",
        "\n",
        "if TYPE_WEIGHT == \"median-frequency\" or TYPE_WEIGHT == \"inverse\" or TYPE_WEIGHT == \"log\":\n",
        "  loss_fn = PIDNetLoss(threshold=0.8, class_weights=class_weights)\n",
        "  mix_loss = CrossEntropyLoss2dPixelWiseWeighted(ignore_index = IGNORE_INDEX, weight = torch.tensor(class_weights).cuda())\n",
        "else:\n",
        "  loss_fn = PIDNetLoss(threshold=0.8, class_weights=None)\n",
        "  mix_loss = CrossEntropyLoss2dPixelWiseWeighted(ignore_index = IGNORE_INDEX, weight = None)\n",
        "\n",
        "print(device)\n",
        "\n",
        "print(len(target_loader))\n",
        "print(len(source_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definition ema model"
      ],
      "metadata": {
        "id": "PBJn-0ZMR-WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_ema_model(model):\n",
        "    \"\"\"Returns a new model that is used to generate pseudo-labels\"\"\"\n",
        "\n",
        "    ema_model = get_seg_model(cfg, imgnet_pretrained=True)\n",
        "\n",
        "    for param in ema_model.parameters():\n",
        "        param.detach_()\n",
        "    mp = list(model.parameters())\n",
        "    mcp = list(ema_model.parameters())\n",
        "    n = len(mp)\n",
        "    for i in range(0, n):\n",
        "        mcp[i].data[:] = mp[i].data[:].clone()\n",
        "\n",
        "    return ema_model\n",
        "\n",
        "\n",
        "def update_ema_variables(ema_model, model, alpha_teacher, iteration):\n",
        "    # Use the \"true\" average until the exponential average is more correct\n",
        "    alpha_teacher = min(1 - 1 / (iteration + 1), alpha_teacher)\n",
        "\n",
        "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
        "        ema_param.data[:] = alpha_teacher * ema_param[:].data[:] + (1 - alpha_teacher) * param[:].data[:]\n",
        "    return ema_model"
      ],
      "metadata": {
        "id": "kxmHtcIQR3N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_class_mask(pred, classes):\n",
        "    pred, classes = torch.broadcast_tensors(pred.unsqueeze(0), classes.unsqueeze(1).unsqueeze(2))\n",
        "    N = pred.eq(classes).sum(0)\n",
        "    return N"
      ],
      "metadata": {
        "id": "E0WePh8KaIUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oneMix(mask, data = None, target = None):\n",
        "    #Mix\n",
        "    if not (data is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], data[0])\n",
        "        data = (stackedMask0*data[0]+(1-stackedMask0)*data[1]).unsqueeze(0)\n",
        "    if not (target is None):\n",
        "        stackedMask0, _ = torch.broadcast_tensors(mask[0], target[0])\n",
        "        target = (stackedMask0*target[0]+(1-stackedMask0)*target[1]).unsqueeze(0)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "kZ1q-SXrczgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strong_transform(target, masks_mix, aug = None, data = None):\n",
        "    data, target = oneMix(masks_mix, data, target)\n",
        "\n",
        "    if data is not None:\n",
        "      data_np = data.squeeze(0).cpu().numpy()\n",
        "    target_np = target.squeeze(0).cpu().numpy()\n",
        "\n",
        "    if data is not None:\n",
        "      data_np = np.transpose(data_np, (1, 2, 0))\n",
        "    target_np = np.transpose(target_np, (1, 2, 0))\n",
        "\n",
        "    if data is not None:\n",
        "      augmented = aug(image=data_np, mask=target_np)\n",
        "\n",
        "      data = augmented['image']\n",
        "      target = augmented['mask']\n",
        "\n",
        "      data = torch.from_numpy(data).permute(2, 0, 1).unsqueeze(0)  # (1, C, H, W)\n",
        "      target = torch.from_numpy(target).squeeze(-1).unsqueeze(0)  # (1, H, W)\n",
        "    else:\n",
        "      target = torch.from_numpy(target_np).squeeze(-1).unsqueeze(0)  # (1, H, W)\n",
        "      return None, target\n",
        "\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "KiJJDkIFcOpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "jOCaZndAkPy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjNWfPicTLQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453d3922-ee40-441d-9b0b-56845a2e4c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a9464f4ede7a>:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=27.2879]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 1.7055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.57it/s, Val_Loss=9.5177, mIoU=0.1619]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.6180\n",
            "→ Overall mIoU: 0.1619\n",
            "  → background IoU: 0.1501\n",
            "  → building IoU: 0.2198\n",
            "  → road IoU: 0.2425\n",
            "  → water IoU: 0.1780\n",
            "  → barren IoU: 0.1650\n",
            "  → forest IoU: 0.1413\n",
            "  → agriculture IoU: 0.0366\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=16.8191]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 1.0512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.63it/s, Val_Loss=7.4878, mIoU=0.1825]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4862\n",
            "→ Overall mIoU: 0.1825\n",
            "  → background IoU: 0.0910\n",
            "  → building IoU: 0.2473\n",
            "  → road IoU: 0.2954\n",
            "  → water IoU: 0.2064\n",
            "  → barren IoU: 0.1617\n",
            "  → forest IoU: 0.2398\n",
            "  → agriculture IoU: 0.0356\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=14.1158]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.8822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.69it/s, Val_Loss=7.1488, mIoU=0.2165]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4642\n",
            "→ Overall mIoU: 0.2165\n",
            "  → background IoU: 0.1617\n",
            "  → building IoU: 0.3078\n",
            "  → road IoU: 0.3457\n",
            "  → water IoU: 0.2894\n",
            "  → barren IoU: 0.1172\n",
            "  → forest IoU: 0.2531\n",
            "  → agriculture IoU: 0.0407\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=13.2572]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.8286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=6.3224, mIoU=0.2377]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4105\n",
            "→ Overall mIoU: 0.2377\n",
            "  → background IoU: 0.1429\n",
            "  → building IoU: 0.3409\n",
            "  → road IoU: 0.3480\n",
            "  → water IoU: 0.3395\n",
            "  → barren IoU: 0.1720\n",
            "  → forest IoU: 0.2721\n",
            "  → agriculture IoU: 0.0483\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=12.3219]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=6.0479, mIoU=0.2487]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3927\n",
            "→ Overall mIoU: 0.2487\n",
            "  → background IoU: 0.1665\n",
            "  → building IoU: 0.3580\n",
            "  → road IoU: 0.3971\n",
            "  → water IoU: 0.3713\n",
            "  → barren IoU: 0.1397\n",
            "  → forest IoU: 0.2649\n",
            "  → agriculture IoU: 0.0430\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.35it/s, Loss_seg=11.6285]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=6.3995, mIoU=0.2534]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4156\n",
            "→ Overall mIoU: 0.2534\n",
            "  → background IoU: 0.1269\n",
            "  → building IoU: 0.3636\n",
            "  → road IoU: 0.3822\n",
            "  → water IoU: 0.3905\n",
            "  → barren IoU: 0.1723\n",
            "  → forest IoU: 0.2930\n",
            "  → agriculture IoU: 0.0451\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=11.0471]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=6.0030, mIoU=0.2781]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3898\n",
            "→ Overall mIoU: 0.2781\n",
            "  → background IoU: 0.1325\n",
            "  → building IoU: 0.3722\n",
            "  → road IoU: 0.4071\n",
            "  → water IoU: 0.4696\n",
            "  → barren IoU: 0.2241\n",
            "  → forest IoU: 0.2956\n",
            "  → agriculture IoU: 0.0454\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=10.6414]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=5.3655, mIoU=0.3076]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3484\n",
            "→ Overall mIoU: 0.3076\n",
            "  → background IoU: 0.2056\n",
            "  → building IoU: 0.3986\n",
            "  → road IoU: 0.4081\n",
            "  → water IoU: 0.4809\n",
            "  → barren IoU: 0.2466\n",
            "  → forest IoU: 0.3434\n",
            "  → agriculture IoU: 0.0702\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=10.0049]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.70it/s, Val_Loss=5.5733, mIoU=0.3052]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3619\n",
            "→ Overall mIoU: 0.3052\n",
            "  → background IoU: 0.2241\n",
            "  → building IoU: 0.3970\n",
            "  → road IoU: 0.4096\n",
            "  → water IoU: 0.4375\n",
            "  → barren IoU: 0.2398\n",
            "  → forest IoU: 0.3532\n",
            "  → agriculture IoU: 0.0749\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=10.0869]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.69it/s, Val_Loss=5.5707, mIoU=0.2853]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3617\n",
            "→ Overall mIoU: 0.2853\n",
            "  → background IoU: 0.2381\n",
            "  → building IoU: 0.3995\n",
            "  → road IoU: 0.4089\n",
            "  → water IoU: 0.4053\n",
            "  → barren IoU: 0.1021\n",
            "  → forest IoU: 0.3579\n",
            "  → agriculture IoU: 0.0852\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=9.7284]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6080\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=5.0183, mIoU=0.3397]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3259\n",
            "→ Overall mIoU: 0.3397\n",
            "  → background IoU: 0.3057\n",
            "  → building IoU: 0.4215\n",
            "  → road IoU: 0.4269\n",
            "  → water IoU: 0.5162\n",
            "  → barren IoU: 0.2050\n",
            "  → forest IoU: 0.3906\n",
            "  → agriculture IoU: 0.1121\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=9.4316]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.64it/s, Val_Loss=5.4220, mIoU=0.3303]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3521\n",
            "→ Overall mIoU: 0.3303\n",
            "  → background IoU: 0.2419\n",
            "  → building IoU: 0.4174\n",
            "  → road IoU: 0.4384\n",
            "  → water IoU: 0.4591\n",
            "  → barren IoU: 0.2695\n",
            "  → forest IoU: 0.3848\n",
            "  → agriculture IoU: 0.1010\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.2866]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.62it/s, Val_Loss=5.5624, mIoU=0.3294]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3612\n",
            "→ Overall mIoU: 0.3294\n",
            "  → background IoU: 0.2330\n",
            "  → building IoU: 0.4179\n",
            "  → road IoU: 0.4535\n",
            "  → water IoU: 0.4994\n",
            "  → barren IoU: 0.2381\n",
            "  → forest IoU: 0.3835\n",
            "  → agriculture IoU: 0.0804\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=8.9900]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=4.8986, mIoU=0.3482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3181\n",
            "→ Overall mIoU: 0.3482\n",
            "  → background IoU: 0.3174\n",
            "  → building IoU: 0.4264\n",
            "  → road IoU: 0.4653\n",
            "  → water IoU: 0.5162\n",
            "  → barren IoU: 0.2161\n",
            "  → forest IoU: 0.3842\n",
            "  → agriculture IoU: 0.1118\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=9.0197]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=5.4754, mIoU=0.3346]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3555\n",
            "→ Overall mIoU: 0.3346\n",
            "  → background IoU: 0.3085\n",
            "  → building IoU: 0.4300\n",
            "  → road IoU: 0.4388\n",
            "  → water IoU: 0.4702\n",
            "  → barren IoU: 0.1621\n",
            "  → forest IoU: 0.4018\n",
            "  → agriculture IoU: 0.1308\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.35it/s, Loss_seg=8.7185]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=4.7908, mIoU=0.3604]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3111\n",
            "→ Overall mIoU: 0.3604\n",
            "  → background IoU: 0.3341\n",
            "  → building IoU: 0.4450\n",
            "  → road IoU: 0.4582\n",
            "  → water IoU: 0.5634\n",
            "  → barren IoU: 0.1869\n",
            "  → forest IoU: 0.4092\n",
            "  → agriculture IoU: 0.1258\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 [Training]: 100%|██████████| 58/58 [00:42<00:00,  1.35it/s, Loss_seg=8.6595]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 17 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.61it/s, Val_Loss=5.2646, mIoU=0.3501]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3419\n",
            "→ Overall mIoU: 0.3501\n",
            "  → background IoU: 0.2789\n",
            "  → building IoU: 0.4367\n",
            "  → road IoU: 0.4579\n",
            "  → water IoU: 0.5003\n",
            "  → barren IoU: 0.2636\n",
            "  → forest IoU: 0.4140\n",
            "  → agriculture IoU: 0.0993\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=8.5076]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.62it/s, Val_Loss=4.7342, mIoU=0.3685]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3074\n",
            "→ Overall mIoU: 0.3685\n",
            "  → background IoU: 0.3311\n",
            "  → building IoU: 0.4401\n",
            "  → road IoU: 0.4605\n",
            "  → water IoU: 0.4938\n",
            "  → barren IoU: 0.3181\n",
            "  → forest IoU: 0.4029\n",
            "  → agriculture IoU: 0.1330\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.35it/s, Loss_seg=8.5066]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.69it/s, Val_Loss=5.0203, mIoU=0.3497]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3260\n",
            "→ Overall mIoU: 0.3497\n",
            "  → background IoU: 0.3183\n",
            "  → building IoU: 0.4481\n",
            "  → road IoU: 0.4491\n",
            "  → water IoU: 0.5194\n",
            "  → barren IoU: 0.1781\n",
            "  → forest IoU: 0.4007\n",
            "  → agriculture IoU: 0.1341\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=8.3471]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=5.1093, mIoU=0.3566]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3318\n",
            "→ Overall mIoU: 0.3566\n",
            "  → background IoU: 0.3173\n",
            "  → building IoU: 0.4512\n",
            "  → road IoU: 0.4689\n",
            "  → water IoU: 0.4939\n",
            "  → barren IoU: 0.2095\n",
            "  → forest IoU: 0.4196\n",
            "  → agriculture IoU: 0.1355\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=26.7905]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 1.6744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=9.2789, mIoU=0.1578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.6025\n",
            "→ Overall mIoU: 0.1578\n",
            "  → background IoU: 0.1173\n",
            "  → building IoU: 0.2484\n",
            "  → road IoU: 0.2336\n",
            "  → water IoU: 0.1963\n",
            "  → barren IoU: 0.1586\n",
            "  → forest IoU: 0.0892\n",
            "  → agriculture IoU: 0.0614\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=16.4143]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 1.0259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.70it/s, Val_Loss=7.2940, mIoU=0.2146]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4736\n",
            "→ Overall mIoU: 0.2146\n",
            "  → background IoU: 0.2227\n",
            "  → building IoU: 0.2662\n",
            "  → road IoU: 0.2905\n",
            "  → water IoU: 0.2438\n",
            "  → barren IoU: 0.2489\n",
            "  → forest IoU: 0.1118\n",
            "  → agriculture IoU: 0.1180\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=13.3549]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.8347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=6.1130, mIoU=0.2421]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3969\n",
            "→ Overall mIoU: 0.2421\n",
            "  → background IoU: 0.1610\n",
            "  → building IoU: 0.3092\n",
            "  → road IoU: 0.3351\n",
            "  → water IoU: 0.3376\n",
            "  → barren IoU: 0.2677\n",
            "  → forest IoU: 0.1863\n",
            "  → agriculture IoU: 0.0979\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=11.9232]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=5.8559, mIoU=0.2502]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3803\n",
            "→ Overall mIoU: 0.2502\n",
            "  → background IoU: 0.1530\n",
            "  → building IoU: 0.3393\n",
            "  → road IoU: 0.3572\n",
            "  → water IoU: 0.3500\n",
            "  → barren IoU: 0.2963\n",
            "  → forest IoU: 0.1485\n",
            "  → agriculture IoU: 0.1072\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=11.2302]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=5.5391, mIoU=0.2754]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3597\n",
            "→ Overall mIoU: 0.2754\n",
            "  → background IoU: 0.1785\n",
            "  → building IoU: 0.3508\n",
            "  → road IoU: 0.3519\n",
            "  → water IoU: 0.4095\n",
            "  → barren IoU: 0.3242\n",
            "  → forest IoU: 0.1918\n",
            "  → agriculture IoU: 0.1207\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=10.7854]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.69it/s, Val_Loss=5.5636, mIoU=0.2665]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3613\n",
            "→ Overall mIoU: 0.2665\n",
            "  → background IoU: 0.1531\n",
            "  → building IoU: 0.3534\n",
            "  → road IoU: 0.3753\n",
            "  → water IoU: 0.4109\n",
            "  → barren IoU: 0.3185\n",
            "  → forest IoU: 0.1581\n",
            "  → agriculture IoU: 0.0965\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.31it/s, Loss_seg=10.4078]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=5.2446, mIoU=0.2785]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3406\n",
            "→ Overall mIoU: 0.2785\n",
            "  → background IoU: 0.1735\n",
            "  → building IoU: 0.3653\n",
            "  → road IoU: 0.3849\n",
            "  → water IoU: 0.4450\n",
            "  → barren IoU: 0.3240\n",
            "  → forest IoU: 0.1542\n",
            "  → agriculture IoU: 0.1023\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.30it/s, Loss_seg=10.0433]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.69it/s, Val_Loss=5.0268, mIoU=0.3014]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3264\n",
            "→ Overall mIoU: 0.3014\n",
            "  → background IoU: 0.1900\n",
            "  → building IoU: 0.3770\n",
            "  → road IoU: 0.4173\n",
            "  → water IoU: 0.5150\n",
            "  → barren IoU: 0.3297\n",
            "  → forest IoU: 0.1873\n",
            "  → agriculture IoU: 0.0936\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=9.7945]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=5.0195, mIoU=0.2889]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3259\n",
            "→ Overall mIoU: 0.2889\n",
            "  → background IoU: 0.1683\n",
            "  → building IoU: 0.3795\n",
            "  → road IoU: 0.4094\n",
            "  → water IoU: 0.4630\n",
            "  → barren IoU: 0.3088\n",
            "  → forest IoU: 0.1895\n",
            "  → agriculture IoU: 0.1041\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=9.4648]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=4.9667, mIoU=0.3024]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3225\n",
            "→ Overall mIoU: 0.3024\n",
            "  → background IoU: 0.2262\n",
            "  → building IoU: 0.3825\n",
            "  → road IoU: 0.4278\n",
            "  → water IoU: 0.4776\n",
            "  → barren IoU: 0.3563\n",
            "  → forest IoU: 0.1455\n",
            "  → agriculture IoU: 0.1005\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=9.2634]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.63it/s, Val_Loss=4.9114, mIoU=0.3017]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3189\n",
            "→ Overall mIoU: 0.3017\n",
            "  → background IoU: 0.1911\n",
            "  → building IoU: 0.3945\n",
            "  → road IoU: 0.4166\n",
            "  → water IoU: 0.4777\n",
            "  → barren IoU: 0.3526\n",
            "  → forest IoU: 0.1702\n",
            "  → agriculture IoU: 0.1095\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.0609]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=4.8466, mIoU=0.3003]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3147\n",
            "→ Overall mIoU: 0.3003\n",
            "  → background IoU: 0.1771\n",
            "  → building IoU: 0.3940\n",
            "  → road IoU: 0.4104\n",
            "  → water IoU: 0.4660\n",
            "  → barren IoU: 0.3377\n",
            "  → forest IoU: 0.2068\n",
            "  → agriculture IoU: 0.1102\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=8.8722]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=4.9075, mIoU=0.3170]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3187\n",
            "→ Overall mIoU: 0.3170\n",
            "  → background IoU: 0.2141\n",
            "  → building IoU: 0.3999\n",
            "  → road IoU: 0.4179\n",
            "  → water IoU: 0.5272\n",
            "  → barren IoU: 0.3555\n",
            "  → forest IoU: 0.1958\n",
            "  → agriculture IoU: 0.1082\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=8.7723]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.71it/s, Val_Loss=4.6467, mIoU=0.3327]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3017\n",
            "→ Overall mIoU: 0.3327\n",
            "  → background IoU: 0.2146\n",
            "  → building IoU: 0.4029\n",
            "  → road IoU: 0.4332\n",
            "  → water IoU: 0.5184\n",
            "  → barren IoU: 0.3503\n",
            "  → forest IoU: 0.2804\n",
            "  → agriculture IoU: 0.1292\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=8.6294]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=4.7314, mIoU=0.3309]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3072\n",
            "→ Overall mIoU: 0.3309\n",
            "  → background IoU: 0.2303\n",
            "  → building IoU: 0.4003\n",
            "  → road IoU: 0.4452\n",
            "  → water IoU: 0.5080\n",
            "  → barren IoU: 0.3370\n",
            "  → forest IoU: 0.2617\n",
            "  → agriculture IoU: 0.1339\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.31it/s, Loss_seg=8.4760]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.70it/s, Val_Loss=4.6721, mIoU=0.3336]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3034\n",
            "→ Overall mIoU: 0.3336\n",
            "  → background IoU: 0.2317\n",
            "  → building IoU: 0.4047\n",
            "  → road IoU: 0.4401\n",
            "  → water IoU: 0.5357\n",
            "  → barren IoU: 0.3505\n",
            "  → forest IoU: 0.2404\n",
            "  → agriculture IoU: 0.1321\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=8.6405]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 17 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.70it/s, Val_Loss=4.6894, mIoU=0.3368]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3045\n",
            "→ Overall mIoU: 0.3368\n",
            "  → background IoU: 0.2178\n",
            "  → building IoU: 0.4073\n",
            "  → road IoU: 0.4462\n",
            "  → water IoU: 0.5341\n",
            "  → barren IoU: 0.3451\n",
            "  → forest IoU: 0.2684\n",
            "  → agriculture IoU: 0.1385\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=8.4932]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.61it/s, Val_Loss=4.7197, mIoU=0.3433]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3065\n",
            "→ Overall mIoU: 0.3433\n",
            "  → background IoU: 0.2488\n",
            "  → building IoU: 0.4042\n",
            "  → road IoU: 0.4429\n",
            "  → water IoU: 0.5167\n",
            "  → barren IoU: 0.3400\n",
            "  → forest IoU: 0.2957\n",
            "  → agriculture IoU: 0.1552\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.31it/s, Loss_seg=8.4777]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.61it/s, Val_Loss=4.6459, mIoU=0.3549]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_5_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3017\n",
            "→ Overall mIoU: 0.3549\n",
            "  → background IoU: 0.2787\n",
            "  → building IoU: 0.4063\n",
            "  → road IoU: 0.4603\n",
            "  → water IoU: 0.5331\n",
            "  → barren IoU: 0.3386\n",
            "  → forest IoU: 0.2966\n",
            "  → agriculture IoU: 0.1710\n",
            "[0.000164829272419854]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=8.3493]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.70it/s, Val_Loss=4.6361, mIoU=0.3411]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3010\n",
            "→ Overall mIoU: 0.3411\n",
            "  → background IoU: 0.2312\n",
            "  → building IoU: 0.4116\n",
            "  → road IoU: 0.4520\n",
            "  → water IoU: 0.5251\n",
            "  → barren IoU: 0.3577\n",
            "  → forest IoU: 0.2676\n",
            "  → agriculture IoU: 0.1425\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=27.9537]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 1.7471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 1 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=9.1497, mIoU=0.1663]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.5941\n",
            "→ Overall mIoU: 0.1663\n",
            "  → background IoU: 0.1852\n",
            "  → building IoU: 0.2155\n",
            "  → road IoU: 0.2087\n",
            "  → water IoU: 0.2432\n",
            "  → barren IoU: 0.1650\n",
            "  → forest IoU: 0.1100\n",
            "  → agriculture IoU: 0.0366\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.30it/s, Loss_seg=17.6723]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 1.1045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=7.2722, mIoU=0.2208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4722\n",
            "→ Overall mIoU: 0.2208\n",
            "  → background IoU: 0.1867\n",
            "  → building IoU: 0.2223\n",
            "  → road IoU: 0.2436\n",
            "  → water IoU: 0.3702\n",
            "  → barren IoU: 0.2702\n",
            "  → forest IoU: 0.2011\n",
            "  → agriculture IoU: 0.0517\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=14.3735]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.8983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=6.4370, mIoU=0.2334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.4180\n",
            "→ Overall mIoU: 0.2334\n",
            "  → background IoU: 0.1207\n",
            "  → building IoU: 0.2868\n",
            "  → road IoU: 0.2824\n",
            "  → water IoU: 0.3363\n",
            "  → barren IoU: 0.2872\n",
            "  → forest IoU: 0.2710\n",
            "  → agriculture IoU: 0.0495\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=12.7540]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=6.0467, mIoU=0.2828]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3926\n",
            "→ Overall mIoU: 0.2828\n",
            "  → background IoU: 0.1962\n",
            "  → building IoU: 0.3308\n",
            "  → road IoU: 0.3219\n",
            "  → water IoU: 0.4373\n",
            "  → barren IoU: 0.3409\n",
            "  → forest IoU: 0.2870\n",
            "  → agriculture IoU: 0.0657\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.31it/s, Loss_seg=12.1814]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.62it/s, Val_Loss=5.4384, mIoU=0.2831]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3531\n",
            "→ Overall mIoU: 0.2831\n",
            "  → background IoU: 0.1426\n",
            "  → building IoU: 0.3637\n",
            "  → road IoU: 0.3497\n",
            "  → water IoU: 0.3823\n",
            "  → barren IoU: 0.3150\n",
            "  → forest IoU: 0.3340\n",
            "  → agriculture IoU: 0.0946\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=11.5467]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.7217\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.64it/s, Val_Loss=5.2680, mIoU=0.3083]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3421\n",
            "→ Overall mIoU: 0.3083\n",
            "  → background IoU: 0.2153\n",
            "  → building IoU: 0.3490\n",
            "  → road IoU: 0.3547\n",
            "  → water IoU: 0.4548\n",
            "  → barren IoU: 0.3557\n",
            "  → forest IoU: 0.3451\n",
            "  → agriculture IoU: 0.0833\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=11.0562]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=5.0857, mIoU=0.3202]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3302\n",
            "→ Overall mIoU: 0.3202\n",
            "  → background IoU: 0.1742\n",
            "  → building IoU: 0.3762\n",
            "  → road IoU: 0.3584\n",
            "  → water IoU: 0.5084\n",
            "  → barren IoU: 0.3679\n",
            "  → forest IoU: 0.3671\n",
            "  → agriculture IoU: 0.0889\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=10.6562]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=4.8884, mIoU=0.3273]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3174\n",
            "→ Overall mIoU: 0.3273\n",
            "  → background IoU: 0.1844\n",
            "  → building IoU: 0.3937\n",
            "  → road IoU: 0.3837\n",
            "  → water IoU: 0.4849\n",
            "  → barren IoU: 0.3652\n",
            "  → forest IoU: 0.3811\n",
            "  → agriculture IoU: 0.0978\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=10.4434]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=4.7971, mIoU=0.3356]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3115\n",
            "→ Overall mIoU: 0.3356\n",
            "  → background IoU: 0.1809\n",
            "  → building IoU: 0.4000\n",
            "  → road IoU: 0.4004\n",
            "  → water IoU: 0.5159\n",
            "  → barren IoU: 0.3681\n",
            "  → forest IoU: 0.3792\n",
            "  → agriculture IoU: 0.1046\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.32it/s, Loss_seg=10.0424]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=4.7442, mIoU=0.3408]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3081\n",
            "→ Overall mIoU: 0.3408\n",
            "  → background IoU: 0.1880\n",
            "  → building IoU: 0.4023\n",
            "  → road IoU: 0.4124\n",
            "  → water IoU: 0.5085\n",
            "  → barren IoU: 0.3744\n",
            "  → forest IoU: 0.3831\n",
            "  → agriculture IoU: 0.1166\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 [Training]: 100%|██████████| 58/58 [00:44<00:00,  1.31it/s, Loss_seg=9.8138]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.67it/s, Val_Loss=4.7019, mIoU=0.3393]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.3053\n",
            "→ Overall mIoU: 0.3393\n",
            "  → background IoU: 0.1849\n",
            "  → building IoU: 0.4072\n",
            "  → road IoU: 0.4186\n",
            "  → water IoU: 0.5121\n",
            "  → barren IoU: 0.3449\n",
            "  → forest IoU: 0.3818\n",
            "  → agriculture IoU: 0.1257\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.7769]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.64it/s, Val_Loss=4.9189, mIoU=0.3507]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.3194\n",
            "→ Overall mIoU: 0.3507\n",
            "  → background IoU: 0.2704\n",
            "  → building IoU: 0.3702\n",
            "  → road IoU: 0.4242\n",
            "  → water IoU: 0.5458\n",
            "  → barren IoU: 0.3798\n",
            "  → forest IoU: 0.3730\n",
            "  → agriculture IoU: 0.0913\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=9.6346]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.6022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.61it/s, Val_Loss=4.5010, mIoU=0.3563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.2923\n",
            "→ Overall mIoU: 0.3563\n",
            "  → background IoU: 0.1927\n",
            "  → building IoU: 0.4220\n",
            "  → road IoU: 0.4229\n",
            "  → water IoU: 0.5707\n",
            "  → barren IoU: 0.3632\n",
            "  → forest IoU: 0.4012\n",
            "  → agriculture IoU: 0.1218\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.5349]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.63it/s, Val_Loss=4.5744, mIoU=0.3448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.2970\n",
            "→ Overall mIoU: 0.3448\n",
            "  → background IoU: 0.1634\n",
            "  → building IoU: 0.4169\n",
            "  → road IoU: 0.4217\n",
            "  → water IoU: 0.5618\n",
            "  → barren IoU: 0.3837\n",
            "  → forest IoU: 0.3516\n",
            "  → agriculture IoU: 0.1146\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.4257]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 15 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.69it/s, Val_Loss=4.5041, mIoU=0.3610]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.2925\n",
            "→ Overall mIoU: 0.3610\n",
            "  → background IoU: 0.1912\n",
            "  → building IoU: 0.4218\n",
            "  → road IoU: 0.4211\n",
            "  → water IoU: 0.5597\n",
            "  → barren IoU: 0.3814\n",
            "  → forest IoU: 0.4093\n",
            "  → agriculture IoU: 0.1424\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.5185]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.68it/s, Val_Loss=4.4336, mIoU=0.3683]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.2879\n",
            "→ Overall mIoU: 0.3683\n",
            "  → background IoU: 0.2270\n",
            "  → building IoU: 0.4263\n",
            "  → road IoU: 0.4355\n",
            "  → water IoU: 0.5650\n",
            "  → barren IoU: 0.3713\n",
            "  → forest IoU: 0.4079\n",
            "  → agriculture IoU: 0.1452\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.32it/s, Loss_seg=9.0064]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 17 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.62it/s, Val_Loss=4.3922, mIoU=0.3683]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.2852\n",
            "→ Overall mIoU: 0.3683\n",
            "  → background IoU: 0.2208\n",
            "  → building IoU: 0.4286\n",
            "  → road IoU: 0.4392\n",
            "  → water IoU: 0.5538\n",
            "  → barren IoU: 0.3885\n",
            "  → forest IoU: 0.4151\n",
            "  → agriculture IoU: 0.1324\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=9.1800]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.66it/s, Val_Loss=4.4033, mIoU=0.3660]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.2859\n",
            "→ Overall mIoU: 0.3660\n",
            "  → background IoU: 0.2077\n",
            "  → building IoU: 0.4309\n",
            "  → road IoU: 0.4395\n",
            "  → water IoU: 0.5771\n",
            "  → barren IoU: 0.3732\n",
            "  → forest IoU: 0.4116\n",
            "  → agriculture IoU: 0.1216\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.34it/s, Loss_seg=9.1121]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5695\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.64it/s, Val_Loss=4.3576, mIoU=0.3746]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello con miou migliore salvato: /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_6_class.pth\n",
            "\n",
            "→ Validation Loss: 0.2830\n",
            "→ Overall mIoU: 0.3746\n",
            "  → background IoU: 0.2474\n",
            "  → building IoU: 0.4261\n",
            "  → road IoU: 0.4464\n",
            "  → water IoU: 0.5763\n",
            "  → barren IoU: 0.3685\n",
            "  → forest IoU: 0.4171\n",
            "  → agriculture IoU: 0.1406\n",
            "[0.000329658544839708]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 [Training]: 100%|██████████| 58/58 [00:43<00:00,  1.33it/s, Loss_seg=8.9321]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/20 Summary\n",
            "  → Segmentation Source Loss (RAW) : 0.5583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20 [Validation]: 100%|██████████| 15/15 [00:04<00:00,  3.65it/s, Val_Loss=4.4066, mIoU=0.3736]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "→ Validation Loss: 0.2861\n",
            "→ Overall mIoU: 0.3736\n",
            "  → background IoU: 0.2716\n",
            "  → building IoU: 0.4264\n",
            "  → road IoU: 0.4325\n",
            "  → water IoU: 0.5579\n",
            "  → barren IoU: 0.3815\n",
            "  → forest IoU: 0.4150\n",
            "  → agriculture IoU: 0.1303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torchmetrics.segmentation import MeanIoU\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "\n",
        "# Loop over each class to be blocked (excluded) during training\n",
        "for index_of_blocked_class in range(NUM_CLASSES):\n",
        "\n",
        "    # Initialize segmentation model with ImageNet pretrained weights\n",
        "    model = get_seg_model(cfg, imgnet_pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer with SGD, using given learning rate and momentum\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM)\n",
        "\n",
        "    # Define learning rate scheduler that reduces LR on plateau of metric (maximizing mIoU)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=GAMMA, patience=3, threshold=0.01)\n",
        "\n",
        "    # Choose loss functions based on weighting type\n",
        "    if TYPE_WEIGHT in [\"median-frequency\", \"inverse\", \"log\"]:\n",
        "        loss_fn = PIDNetLoss(threshold=0.8, class_weights=class_weights)\n",
        "        mix_loss = CrossEntropyLoss2dPixelWiseWeighted(ignore_index=IGNORE_INDEX,\n",
        "                                                      weight=torch.tensor(class_weights).cuda())\n",
        "    else:\n",
        "        loss_fn = PIDNetLoss(threshold=0.8, class_weights=None)\n",
        "        mix_loss = CrossEntropyLoss2dPixelWiseWeighted(ignore_index=IGNORE_INDEX, weight=None)\n",
        "\n",
        "    # Create an exponential moving average (EMA) version of the model for target predictions\n",
        "    ema_model = create_ema_model(model)\n",
        "    ema_model.to(device)\n",
        "\n",
        "    # Define Mean Intersection over Union (mIoU) metric for validation\n",
        "    num_classes = 7\n",
        "    miou_classes = MeanIoU(num_classes=num_classes, input_format=\"index\", per_class=True).to(device)\n",
        "\n",
        "    record_miou = 0  # To keep track of best mIoU for model saving\n",
        "\n",
        "    # Training loop over epochs\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(scheduler.get_last_lr())\n",
        "        loss_raw_value = 0\n",
        "        total_train_samples = 0\n",
        "\n",
        "        model.train()\n",
        "        ema_model.train()\n",
        "\n",
        "        # Zip source and target data loaders for semi-supervised learning\n",
        "        train_loader = zip(source_loader, target_loader)\n",
        "        num_batches = min(len(source_loader), len(target_loader))\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        pbar = tqdm(enumerate(train_loader), total=num_batches, desc=f\"Epoch {epoch+1} [Training]\")\n",
        "\n",
        "        for i, (source_batch, target_batch) in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Unpack and move source batch data to device\n",
        "            X_source, y_source, boundary_mask_source = source_batch\n",
        "            X_source, y_source, boundary_mask_source = X_source.to(device), y_source.to(device), boundary_mask_source.to(device)\n",
        "\n",
        "            # Unpack and move target batch data to device\n",
        "            X_target, boundary_mask_target = target_batch\n",
        "            X_target, boundary_mask_target = X_target.to(device), boundary_mask_target.to(device)\n",
        "\n",
        "            # Forward pass on source images through model\n",
        "            outputs_s = model(X_source)\n",
        "            pred_p, pred_main, boundary_head = Upscaling(outputs=outputs_s, boundary_mask=boundary_mask_source, model=model)\n",
        "\n",
        "            # Compute supervised loss on source labeled data\n",
        "            loss_labled = loss_fn(pred_p, pred_main, y_source, boundary_head, boundary_mask_source)\n",
        "\n",
        "            if LOSS_TYPE == \"ohem\":\n",
        "                loss_labled = torch.mean(loss_labled)  # Online hard example mining average\n",
        "\n",
        "            # Forward pass on target images through EMA model for pseudo-labeling\n",
        "            outputs_t = ema_model(X_target)\n",
        "            _, pred_main, _ = Upscaling(outputs=outputs_t, boundary_mask=boundary_mask_target, model=ema_model)\n",
        "\n",
        "            # Generate softmax probabilities and pseudo labels for target images\n",
        "            probs_t = torch.softmax(pred_main.detach(), dim=1)\n",
        "            max_probs, pseudo_labels = torch.max(probs_t, dim=1)\n",
        "\n",
        "            MixMasks = []\n",
        "\n",
        "            # For each source image, select a random subset of classes excluding the blocked class,\n",
        "            # then generate class masks for mixing augmentation\n",
        "            for image_i in range(X_source.shape[0]):\n",
        "                classes = torch.unique(y_source[image_i])\n",
        "                classes = classes[classes != -1]\n",
        "                classes = classes[classes != index_of_blocked_class]  # exclude blocked class\n",
        "\n",
        "                nclasses = classes.shape[0]\n",
        "                classes = classes[torch.Tensor(np.random.choice(nclasses, int((nclasses + nclasses % 2) / 2), replace=False)).long()].cuda()\n",
        "\n",
        "                MixMask = generate_class_mask(y_source[image_i], classes).unsqueeze(0).cuda()\n",
        "                MixMasks.append(MixMask)\n",
        "\n",
        "            mixed_imgs = []\n",
        "            mixed_labels = []\n",
        "            mixed_boundary_masks = []\n",
        "\n",
        "            # Apply strong augmentation mixing source and target images/labels with generated masks\n",
        "            for image_i in range(X_source.shape[0]):\n",
        "                data = torch.cat((X_source[image_i].unsqueeze(0), X_target[image_i].unsqueeze(0)))\n",
        "                labels = torch.cat((y_source[image_i].unsqueeze(0), pseudo_labels[image_i].unsqueeze(0)))\n",
        "\n",
        "                data, mask = strong_transform(\n",
        "                    aug=augment,\n",
        "                    data=data,\n",
        "                    target=labels,\n",
        "                    masks_mix=[MixMasks[image_i]]\n",
        "                )\n",
        "\n",
        "                boundary_mask_mix = extract_boundary_mask(mask)\n",
        "\n",
        "                mixed_imgs.append(data)\n",
        "                mixed_labels.append(mask)\n",
        "                mixed_boundary_masks.append(boundary_mask_mix)\n",
        "\n",
        "            # Optionally show images, labels, and boundary masks of the augmented samples in the first batch\n",
        "            if i == 0 and SHOW_IMG:\n",
        "                fig, axs = plt.subplots(2, 3, figsize=(24, 10))\n",
        "                for j in range(2):\n",
        "                    axs[j, 0].imshow(mixed_imgs[j].squeeze().permute(1, 2, 0).cpu().detach().numpy())\n",
        "                    axs[j, 0].set_title(\"IMG\")\n",
        "                    axs[j, 0].axis('off')\n",
        "\n",
        "                    axs[j, 1].imshow(mixed_labels[j].permute(1, 2, 0).cpu().detach().numpy())\n",
        "                    axs[j, 1].set_title(\"Labels\")\n",
        "                    axs[j, 1].axis('off')\n",
        "\n",
        "                    axs[j, 2].imshow(mixed_boundary_masks[j].permute(1, 2, 0).cpu().detach().numpy())\n",
        "                    axs[j, 2].set_title(\"Boundary\")\n",
        "                    axs[j, 2].axis('off')\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            # Concatenate mixed images, labels and boundary masks to form batch tensors\n",
        "            inputs_mix = torch.cat(mixed_imgs, dim=0).to(device)\n",
        "            targets_mix = torch.cat(mixed_labels, dim=0).to(device)\n",
        "            boundary_masks_mix = torch.cat(mixed_boundary_masks, dim=0).to(device)\n",
        "\n",
        "            # Forward pass on mixed inputs\n",
        "            outputs_mix = model(inputs_mix)\n",
        "            pred_p_mix, pred_main_mix, boundary_head_mix = Upscaling(outputs=outputs_mix, boundary_mask=boundary_masks_mix, model=model)\n",
        "\n",
        "            # Compute pixel-wise weight mask for unlabeled data based on confidence thresholding\n",
        "            if PIXEL_WEIGHT == \"threshold_uniform\":\n",
        "                unlabeled_weight = torch.sum(max_probs.ge(0.968).long() == 1).item() / np.size(np.array(targets_mix.cpu()))\n",
        "                pixelWiseWeight = unlabeled_weight * torch.ones(max_probs.shape).cuda()\n",
        "            else:\n",
        "                pixelWiseWeight = max_probs.ge(THRESHOLD).float().cuda()\n",
        "\n",
        "            onesWeights = torch.ones(pixelWiseWeight.shape).cuda()\n",
        "            pixelWiseWeight_mix = []\n",
        "\n",
        "            # Apply the same mixing transformation to the pixel-wise weights\n",
        "            for image_i in range(X_source.shape[0]):\n",
        "                weights_pair = torch.cat((onesWeights[image_i].unsqueeze(0), pixelWiseWeight[image_i].unsqueeze(0)))\n",
        "\n",
        "                _, mixed_weights = strong_transform(\n",
        "                    target=weights_pair,\n",
        "                    masks_mix=[MixMasks[image_i]]\n",
        "                )\n",
        "\n",
        "                pixelWiseWeight_mix.append(mixed_weights)\n",
        "\n",
        "            pixelWiseWeight_mix = torch.cat(pixelWiseWeight_mix, dim=0).to(device)  # shape [B, H, W]\n",
        "\n",
        "            # Compute the mixed loss weighted by pixel-wise confidence\n",
        "            loss_seg_mix = mix_loss(pred_main_mix, targets_mix, pixelWiseWeight_mix)\n",
        "\n",
        "            # Combine supervised loss and mixed (semi-supervised) loss\n",
        "            loss_overall = loss_seg_mix + loss_labled\n",
        "\n",
        "            # Backpropagation and optimizer step\n",
        "            loss_overall.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update EMA model parameters\n",
        "            ema_model = update_ema_variables(ema_model=ema_model, model=model, alpha_teacher=ALPHA_TEACHER,\n",
        "                                             iteration=(epoch * num_batches + i))\n",
        "\n",
        "            loss_raw_value += loss_overall.item()\n",
        "            total_train_samples += X_target.size(0)\n",
        "\n",
        "            # Update progress bar with current loss\n",
        "            pbar.set_postfix({\n",
        "                \"Loss_seg\": f\"{loss_raw_value / (i+1):.4f}\",\n",
        "            })\n",
        "\n",
        "        # Print training loss summary for the epoch\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS} Summary\")\n",
        "        print(f\"  → Segmentation Source Loss (RAW) : {loss_raw_value / total_train_samples:.4f}\")\n",
        "\n",
        "        # ---------------------- VALIDATION ----------------------\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        miou_classes.reset()\n",
        "        total_val_samples = 0\n",
        "\n",
        "        # Validate without gradient computation\n",
        "        with torch.inference_mode():\n",
        "            pbar_val = tqdm(enumerate(val_loader), total=len(val_loader), desc=f\"Epoch {epoch+1} [Validation]\")\n",
        "\n",
        "            for batch, (X_val, y_val, boundary_mask) in pbar_val:\n",
        "                X_val, y_val, boundary_mask = X_val.to(device), y_val.to(device), boundary_mask.to(device)\n",
        "\n",
        "                outputs = model(X_val)\n",
        "                pred_p, pred_main, boundary_head = Upscaling(outputs=outputs, boundary_mask=boundary_mask, model=model)\n",
        "\n",
        "                loss = loss_fn(pred_p, pred_main, y_val, boundary_head, boundary_mask)\n",
        "\n",
        "                if LOSS_TYPE == \"ohem\":\n",
        "                    loss = torch.mean(loss)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                total_val_samples += X_val.size(0)\n",
        "\n",
        "                preds = pred_main.argmax(dim=1)\n",
        "\n",
        "                # Only consider valid labels for metric calculation\n",
        "                valid_mask = (y_val >= 0) & (y_val < num_classes)\n",
        "                preds_flat = preds[valid_mask]\n",
        "                targets_flat = y_val[valid_mask]\n",
        "\n",
        "                miou_classes.update(preds_flat, targets_flat)\n",
        "\n",
        "                # Update validation progress bar with loss and mean IoU\n",
        "                pbar_val.set_postfix({\n",
        "                    \"Val_Loss\": f\"{val_loss / (batch+1):.4f}\",\n",
        "                    \"mIoU\": f\"{miou_classes.compute().mean():.4f}\"\n",
        "                })\n",
        "\n",
        "        # Calculate average validation loss and mIoU per class\n",
        "        avg_val_loss = val_loss / total_val_samples\n",
        "        miou_per_class = miou_classes.compute()\n",
        "        miou = miou_per_class.mean()\n",
        "\n",
        "        # Save model if current mIoU is better than previous best\n",
        "        if record_miou is None or miou > record_miou:\n",
        "            best_model_path = f\"/content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_{index_of_blocked_class}_class.pth\"\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"Best model saved with mIoU: {best_model_path}\")\n",
        "            record_miou = miou\n",
        "\n",
        "        # Print validation loss and per-class IoU\n",
        "        print(f\"\\n→ Validation Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"→ Overall mIoU: {miou:.4f}\")\n",
        "        for i, iou in enumerate(miou_per_class):\n",
        "            class_name = list(sem_class_to_idx.keys())[list(sem_class_to_idx.values()).index(i)]\n",
        "            print(f\"  → {class_name} IoU: {iou:.4f}\")\n",
        "\n",
        "        # Step the scheduler with the mIoU metric\n",
        "        scheduler.step(miou)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ****************************** Test ******************************\n",
        "from torchmetrics.segmentation import MeanIoU\n",
        "\n",
        "num_classes = 7\n",
        "miou_classes = MeanIoU(num_classes=num_classes, input_format=\"index\", per_class=True).to(device)\n",
        "\n",
        "for i in range(num_classes):\n",
        "    model_path = f\"/content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_{i}_class.pth\"\n",
        "\n",
        "    cfg = Config()\n",
        "    model = get_seg_model(cfg, imgnet_pretrained=True)\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Loaded model {i}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {i}: {e}\")\n",
        "        # Uncomment to see checkpoint keys if needed\n",
        "        # print(list(torch.load(model_path).keys())[:5])\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = 0\n",
        "    miou_classes.reset()\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X_val, y_val, boundary_mask) in enumerate(test_loader):\n",
        "            X_val = X_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "            boundary_mask = boundary_mask.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(X_val)\n",
        "\n",
        "            # Upscale outputs and get predictions\n",
        "            pred_p, pred_main, boundary_head = Upscaling(outputs=outputs, boundary_mask=boundary_mask, model=model)\n",
        "\n",
        "            # Show images for the first batch if enabled\n",
        "            if batch == 0 and SHOW_IMG:\n",
        "                fig, axs = plt.subplots(4, 5, figsize=(12, 5))\n",
        "                for j in range(4):\n",
        "                    axs[j, 0].imshow(pred_p[j].cpu().detach().argmax(dim=0).numpy(), cmap='tab20')\n",
        "                    axs[j, 0].set_title(\"Auxiliary Prediction\")\n",
        "                    axs[j, 0].axis('off')\n",
        "\n",
        "                    axs[j, 1].imshow(pred_main[j].cpu().detach().argmax(dim=0).numpy(), cmap='tab20')\n",
        "                    axs[j, 1].set_title(\"Main Prediction\")\n",
        "                    axs[j, 1].axis('off')\n",
        "\n",
        "                    axs[j, 2].imshow(y_val[j].cpu().detach().numpy(), cmap='tab20')\n",
        "                    axs[j, 2].set_title(\"Target Mask\")\n",
        "                    axs[j, 2].axis('off')\n",
        "\n",
        "                    axs[j, 3].imshow(boundary_head[j].cpu().detach().sigmoid().squeeze(0).numpy(), cmap='gray')\n",
        "                    axs[j, 3].set_title(\"Boundary Prediction\")\n",
        "                    axs[j, 3].axis('off')\n",
        "\n",
        "                    axs[j, 4].imshow(X_val[j].cpu().detach().squeeze(0).numpy().transpose(1, 2, 0))\n",
        "                    axs[j, 4].set_title(\"Input Image\")\n",
        "                    axs[j, 4].axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(pred_p, pred_main, y_val, boundary_head, boundary_mask)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            total_val_samples += X_val.size(0)\n",
        "\n",
        "            # Final predictions (class with highest probability)\n",
        "            preds = pred_main.argmax(dim=1)  # Shape: (N, H, W)\n",
        "\n",
        "            # Mask valid pixels (class indices from 0 to num_classes - 1)\n",
        "            valid_mask = (y_val >= 0) & (y_val < num_classes)\n",
        "\n",
        "            # Flatten predictions and targets only on valid pixels\n",
        "            preds_flat = preds[valid_mask]\n",
        "            targets_flat = y_val[valid_mask]\n",
        "\n",
        "            miou_classes.update(preds_flat, targets_flat)\n",
        "\n",
        "    avg_val_loss = val_loss / total_val_samples\n",
        "    miou_per_class = miou_classes.compute()\n",
        "    miou = miou_per_class.mean()\n",
        "\n",
        "    print(f\"Test Loss: {avg_val_loss:.4f} | mIoU: {miou:.4f} | Total test samples: {total_val_samples}\")\n",
        "\n",
        "    # Print IoU per class\n",
        "    for cls_idx, iou in enumerate(miou_per_class):\n",
        "        class_name = list(sem_class_to_idx.keys())[list(sem_class_to_idx.values()).index(cls_idx)]\n",
        "        print(f\"  → {class_name} IoU: {iou:.4f}\")"
      ],
      "metadata": {
        "id": "YPXuMPCoYmBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09810e6d-d6f1-4012-cdd5-b97488223792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a9464f4ede7a>:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n",
            "<ipython-input-25-f6b218e9102e>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model 0\n",
            "Test Loss: 0.3091406144442097 | mIoU: 0.19432131946086884 | Total test samples seen: 992\n",
            "  → background IoU: 0.0002\n",
            "  → building IoU: 0.2167\n",
            "  → road IoU: 0.2228\n",
            "  → water IoU: 0.4929\n",
            "  → barren IoU: 0.0483\n",
            "  → forest IoU: 0.0393\n",
            "  → agriculture IoU: 0.3401\n",
            "loaded model 1\n",
            "Test Loss: 0.3402158353597887 | mIoU: 0.22178225219249725 | Total test samples seen: 992\n",
            "  → background IoU: 0.2020\n",
            "  → building IoU: 0.1776\n",
            "  → road IoU: 0.2481\n",
            "  → water IoU: 0.2487\n",
            "  → barren IoU: 0.0733\n",
            "  → forest IoU: 0.1794\n",
            "  → agriculture IoU: 0.4233\n",
            "loaded model 2\n",
            "Test Loss: 0.36043677839540667 | mIoU: 0.2314247190952301 | Total test samples seen: 992\n",
            "  → background IoU: 0.1438\n",
            "  → building IoU: 0.2120\n",
            "  → road IoU: 0.2240\n",
            "  → water IoU: 0.4258\n",
            "  → barren IoU: 0.0263\n",
            "  → forest IoU: 0.1472\n",
            "  → agriculture IoU: 0.4408\n",
            "loaded model 3\n",
            "Test Loss: 0.3119539224332379 | mIoU: 0.23988786339759827 | Total test samples seen: 992\n",
            "  → background IoU: 0.2625\n",
            "  → building IoU: 0.2785\n",
            "  → road IoU: 0.2322\n",
            "  → water IoU: 0.3298\n",
            "  → barren IoU: 0.0849\n",
            "  → forest IoU: 0.1006\n",
            "  → agriculture IoU: 0.3907\n",
            "loaded model 4\n",
            "Test Loss: 0.35471517424429616 | mIoU: 0.24021834135055542 | Total test samples seen: 992\n",
            "  → background IoU: 0.1465\n",
            "  → building IoU: 0.3938\n",
            "  → road IoU: 0.2470\n",
            "  → water IoU: 0.5053\n",
            "  → barren IoU: 0.0427\n",
            "  → forest IoU: 0.0264\n",
            "  → agriculture IoU: 0.3199\n",
            "loaded model 5\n",
            "Test Loss: 0.38264685580807345 | mIoU: 0.1946507841348648 | Total test samples seen: 992\n",
            "  → background IoU: 0.0872\n",
            "  → building IoU: 0.2757\n",
            "  → road IoU: 0.2139\n",
            "  → water IoU: 0.3835\n",
            "  → barren IoU: 0.0255\n",
            "  → forest IoU: 0.0120\n",
            "  → agriculture IoU: 0.3648\n",
            "loaded model 6\n",
            "Test Loss: 0.35370408431176215 | mIoU: 0.22156673669815063 | Total test samples seen: 992\n",
            "  → background IoU: 0.0784\n",
            "  → building IoU: 0.2182\n",
            "  → road IoU: 0.1786\n",
            "  → water IoU: 0.4615\n",
            "  → barren IoU: 0.0621\n",
            "  → forest IoU: 0.1512\n",
            "  → agriculture IoU: 0.4008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model ensemble\n",
        "Take the top 3 models (mIoU) and perform model ensambe"
      ],
      "metadata": {
        "id": "XRHChDSzLdQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 3 models are 2, 3, and 4\n",
        "best_models = [2, 3, 4]\n",
        "\n",
        "model_paths = [f\"/content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_{model_id}_class.pth\" for model_id in best_models]\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "models = []\n",
        "for model_path in model_paths:\n",
        "    model = get_seg_model(cfg, imgnet_pretrained=True)\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "        print(f\"Loaded model at {model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model at {model_path}: {e}\")\n",
        "        # Uncomment to see checkpoint keys if needed\n",
        "        # print(list(torch.load(model_path).keys())[:5])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "\n",
        "miou_classes.reset()\n",
        "total_val_samples = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch, (X_val, y_val, boundary_mask) in enumerate(test_loader):\n",
        "        X_val = X_val.to(device)\n",
        "        y_val = y_val.to(device)\n",
        "        boundary_mask = boundary_mask.to(device)\n",
        "\n",
        "        # List of main predictions from each model\n",
        "        main_pred_list = [model(X_val)[1] for model in models]\n",
        "\n",
        "        # Stack predictions and compute their mean\n",
        "        pred_main_stack = torch.stack(main_pred_list, dim=0)\n",
        "        mean_pred_main = pred_main_stack.mean(dim=0).squeeze(0)\n",
        "\n",
        "        # Upscaling to match ground truth size if needed\n",
        "        h, w = boundary_mask.size(1), boundary_mask.size(2)\n",
        "        ph, pw = mean_pred_main.size(2), mean_pred_main.size(3)\n",
        "        if ph != h or pw != w:\n",
        "            mean_pred_main = F.interpolate(mean_pred_main, size=(h, w), mode='bilinear', align_corners=True)\n",
        "\n",
        "        # Visualize results for the first batch if enabled\n",
        "        if batch == 0 and SHOW_IMG:\n",
        "            fig, axs = plt.subplots(4, 5, figsize=(12, 5))\n",
        "            for j in range(4):\n",
        "                axs[j, 0].imshow(pred_p[j].cpu().detach().argmax(dim=0).numpy(), cmap='tab20')\n",
        "                axs[j, 0].set_title(\"Auxiliary Prediction\")\n",
        "                axs[j, 0].axis('off')\n",
        "\n",
        "                axs[j, 1].imshow(pred_main[j].cpu().detach().argmax(dim=0).numpy(), cmap='tab20')\n",
        "                axs[j, 1].set_title(\"Main Prediction\")\n",
        "                axs[j, 1].axis('off')\n",
        "\n",
        "                axs[j, 2].imshow(y_val[j].cpu().detach().numpy(), cmap='tab20')\n",
        "                axs[j, 2].set_title(\"Target Mask\")\n",
        "                axs[j, 2].axis('off')\n",
        "\n",
        "                axs[j, 3].imshow(boundary_head[j].cpu().detach().sigmoid().squeeze(0).numpy(), cmap='gray')\n",
        "                axs[j, 3].set_title(\"Boundary Prediction\")\n",
        "                axs[j, 3].axis('off')\n",
        "\n",
        "                axs[j, 4].imshow(X_val[j].cpu().detach().squeeze(0).numpy().transpose(1, 2, 0))\n",
        "                axs[j, 4].set_title(\"Input Image\")\n",
        "                axs[j, 4].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        total_val_samples += X_val.size(0)\n",
        "\n",
        "        # Compute predictions by selecting the class with the highest probability\n",
        "        preds = mean_pred_main.argmax(dim=1)  # Shape: (N, H, W)\n",
        "\n",
        "        # Mask valid pixels (class indices between 0 and num_classes-1)\n",
        "        valid_mask = (y_val >= 0) & (y_val < num_classes)\n",
        "\n",
        "        # Flatten predictions and targets on valid pixels only\n",
        "        preds_flat = preds[valid_mask]\n",
        "        targets_flat = y_val[valid_mask]\n",
        "\n",
        "        miou_classes.update(preds_flat, targets_flat)\n",
        "\n",
        "# Compute Mean IoU per class and overall mean\n",
        "miou_per_class = miou_classes.compute()\n",
        "miou = miou_per_class.mean()\n",
        "\n",
        "print(f\"mIoU: {miou:.4f}\")\n",
        "# Per-class IoU\n",
        "for i, iou in enumerate(miou_per_class):\n",
        "    class_name = list(sem_class_to_idx.keys())[list(sem_class_to_idx.values()).index(i)]\n",
        "    print(f\"  → {class_name} IoU: {iou:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apl7aSzALkdk",
        "outputId": "1945cf63-0dc4-4110-ca58-b478187e2609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-a9464f4ede7a>:190: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_state = torch.load(cfg.MODEL.PRETRAINED, map_location='cpu')['state_dict']\n",
            "<ipython-input-26-1dda733c9634>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n",
            "<ipython-input-26-1dda733c9634>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model at /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_2_class.pth\n",
            "loaded model at /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_3_class.pth\n",
            "loaded model at /content/drive/MyDrive/AML_project/checkpoints/best_model_PIDNET_5_DACS_split_without_4_class.pth\n",
            "mIoU: 0.2715633809566498 \n",
            "  → background IoU: 0.2691\n",
            "  → building IoU: 0.3136\n",
            "  → road IoU: 0.2937\n",
            "  → water IoU: 0.4465\n",
            "  → barren IoU: 0.0434\n",
            "  → forest IoU: 0.1115\n",
            "  → agriculture IoU: 0.4232\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "UrjECeMs7Sc5",
        "p_wwWFwFkIoR"
      ],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}